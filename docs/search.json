[
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\nYou should have already downloaded the datasets from Google Classroom and stored them in a common directory on your computer.\nIn this challenge, as in all subsequent challenges, the number of stars corresponds to the difficulty of the dataset. You are only required to do the challenge on one dataset, though you are welcome to do it with multiple datasets.\nIn general, I encourage you to “challenge” yourself by trying to work with a dataset above your experience.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat\\*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 23, 2023\n\n\nChallenge 1 Solution\n\n\nSean Conway\n\n\n\n\nDec 21, 2023\n\n\nChallenge 2 Instructions\n\n\nSean Conway\n\n\n\n\nDec 20, 2023\n\n\ngroup_by() & summarise()\n\n\nSean Conway\n\n\n\n\nDec 18, 2023\n\n\nData Import\n\n\nSean Conway\n\n\n\n\nDec 15, 2023\n\n\nChallenge 1 Instructions\n\n\nSean Conway\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog contains challenges, solutions, and demonstration scripts for DACSS 601."
  },
  {
    "objectID": "posts/example-data_import.html",
    "href": "posts/example-data_import.html",
    "title": "Data Import",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(dplyr)\nlibrary(here)\nlibrary(readxl)\nlibrary(readr)"
  },
  {
    "objectID": "posts/example-data_import.html#overview",
    "href": "posts/example-data_import.html#overview",
    "title": "Data Import",
    "section": "Overview",
    "text": "Overview\nToday, we’re going to read in three versions of the poultry_tidy data. These data are available on the Google Classroom.\nWe will specifically read in 3 data files:\n- poultry_tidy.csv\n- poultry_tidy.xlsx\n- poultry_tidy.RData\nThese are the “clean” versions of the raw data files.\nTo run this file, all 3 datasets should be in the same directory on your computer.\nOn my computer, I have all datasets stored in a folder named _data.\nI also use the here package to manage relative directories."
  },
  {
    "objectID": "posts/example-data_import.html#getting-started",
    "href": "posts/example-data_import.html#getting-started",
    "title": "Data Import",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin, we need to load two packages: readr and readxl, which contain very useful functions for reading in data to `R.\n\nlibrary(readxl)\n\nIf you’re unsure whether or not you have these packages installed, you can run the following command:\n\ninstalled.packages()\n\nWe’re now ready to get started reading in actual datasets."
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-delimited-text-files",
    "href": "posts/example-data_import.html#reading-in-delimited-text-files",
    "title": "Data Import",
    "section": "Reading in delimited text files",
    "text": "Reading in delimited text files\n.csv is a common type of delimited text file. .csv stands for comma-separated value. This means that commas separate cells from one another.\nR has a base read.csv() function. However, it comes with a couple of downsides - namely that it imports data as a dataframe rather than a tibble. So we will be using the function read_csv() from the readr package. In addition to importing data as a tibble, it also does a much better job guessing data types.\nread_csv() is essentially a wrapper function (a function that calls another function) around the more general read_delim() function. Also see read_tsv() for tab-separated values.\n\n?read_delim\n\nLet’s look at the data files available for us to read in:\n\nlist.files(here(\"posts\",\"_data\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"co2_data.txt\"                                                                                    \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"DS0001\"                                                                                          \n[15] \"eggs_tidy.csv\"                                                                                   \n[16] \"emissions.csv\"                                                                                   \n[17] \"End of the Semester Report Fall 2022.csv\"                                                        \n[18] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[19] \"FAOSTAT_country_groups.csv\"                                                                      \n[20] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[21] \"FAOSTAT_livestock.csv\"                                                                           \n[22] \"FedFundsRate.csv\"                                                                                \n[23] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[24] \"hotel_bookings.csv\"                                                                              \n[25] \"NBA_Player_Stats.csv\"                                                                            \n[26] \"online_retail.csv\"                                                                               \n[27] \"organiceggpoultry.xls\"                                                                           \n[28] \"poultry_tidy.csv\"                                                                                \n[29] \"poultry_tidy.RData\"                                                                              \n[30] \"poultry_tidy.xlsx\"                                                                               \n[31] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[32] \"railroad_2012_clean_county.csv\"                                                                  \n[33] \"sce-labor-chart-data-public.xlsx\"                                                                \n[34] \"snl_actors.csv\"                                                                                  \n[35] \"snl_casts.csv\"                                                                                   \n[36] \"snl_seasons.csv\"                                                                                 \n[37] \"starwars1.RData\"                                                                                 \n[38] \"StateCounty2012.xls\"                                                                             \n[39] \"test_objs.RData\"                                                                                 \n[40] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[41] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[42] \"wild_bird_data.xlsx\"                                                                             \n\n\nThere’s a lot of data files there, but we are going to import the poultry_tidy.csv file. Doing so is very simple using read_csv():\n\npoultry_from_csv &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our dataset (to view the tibble, running the name of the object will print it to the console):\n\npoultry_from_csv\n\n\n\n  \n\n\nhead(poultry_from_csv)\n\n\n\n  \n\n\n\nIt worked great! The data is all there. To inspect the data types for each of the four columns in poultry_from_csv, we can use spec() or typeof():\n\npoultry_from_csv &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\n\nspec(poultry_from_csv) # use the spec() function to check the data type for your columns\n\ncols(\n  Product = col_character(),\n  Year = col_double(),\n  Month = col_character(),\n  Price_Dollar = col_double()\n)\n\n# can also use typeof() function on individual columns\ntypeof(poultry_from_csv$Product)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Year)\n\n[1] \"double\"\n\ntypeof(poultry_from_csv$Month)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Price_Dollar)\n\n[1] \"double\"\n\n\nSee this R section below for some more info on read_delim():\n\n# read_delim() has a number of optional arguments\nargs(read_delim)\n\nfunction (file, delim = NULL, quote = \"\\\"\", escape_backslash = FALSE, \n    escape_double = TRUE, col_names = TRUE, col_types = NULL, \n    col_select = NULL, id = NULL, locale = default_locale(), \n    na = c(\"\", \"NA\"), quoted_na = TRUE, comment = \"\", trim_ws = FALSE, \n    skip = 0, n_max = Inf, guess_max = min(1000, n_max), name_repair = \"unique\", \n    num_threads = readr_threads(), progress = show_progress(), \n    show_col_types = should_show_types(), skip_empty_rows = TRUE, \n    lazy = should_read_lazy()) \nNULL\n\n# there's too many to list here, so we will just go over a few\n# run ?read_delim() to learn more\n# 1) delim - text delimiter.\n# default is NULL and read_delim() guesses delimiter\n#\n# 2) quote - symbol telling R when to quote a string\n# default is \"\\\"\"\n# below comes from R documentation on quotes\n# https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html\n# identical() is a function that returns TRUE if two objects are equal\nidentical(1+4, 3+2)\n\n[1] TRUE\n\nidentical('\"It\\'s alive!\", he screamed.',\n          \"\\\"It's alive!\\\", he screamed.\") # same\n\n[1] TRUE\n\n#\n# 3) escape_backlash\n# use backlash to escape special characters?\n# default = FALSE\n#\n# 4) col_names\n# can be TRUE (default), meaning that R reads in the first row of values as column names\n# can FALSE - R creates column names (x1 x2 etc)\n# OR can be a character vector of custom column names\npoultry_custom_cols &lt;- read_csv(\"_data/poultry_tidy.csv\",\n                                col_names = c(\"prod\",\"yr\",\"mo\",\"$\"),\n                                skip = 1) # need this to skip the file's column names\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): prod, mo\ndbl (2): yr, $\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry_custom_cols\n\n\n\n  \n\n\npoultry_custom_cols$`$` # note the backticks around the $ sign\n\n  [1] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [10] 2.38500 2.38500 2.38500 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750\n [19] 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750 3.90500 3.90500 3.90500\n [28] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [37] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n [46] 2.03500 2.03500 2.03500 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250\n [55] 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250 2.35000 2.38500 2.38500\n [64] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [73] 6.37500 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000\n [82] 7.00000 7.03750 7.03750 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [91] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[100] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[109] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.16250 2.16250\n[118] 2.16250 2.16250 2.16250 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000\n[127] 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000 6.37500 6.37500 6.37500\n[136] 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500\n[145] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[154] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[163] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.15000 2.15000 2.15000\n[172] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000\n[181] 2.48000 2.48000 2.48000 2.41500 2.35000 2.35000 2.41500 2.35000 2.35000\n[190] 2.35000 2.35000 2.35000 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[199] 6.45500 6.42300 6.37500 6.37500 6.37500 6.37500 3.90500 3.90500 3.90500\n[208] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[217] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[226] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[235] 2.22000 2.19200 2.15000 2.15000 2.15000 2.15000 2.48000 2.48000 2.48000\n[244] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000\n[253] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[262] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[271] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[280] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[289] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[298] 2.22000 2.22000 2.22000 2.20500 2.20500 2.20500 2.20500 2.20500 2.48000\n[307] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 6.45500 6.45500 6.45500\n[316] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[325] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[334] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[343] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000\n[352] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[361] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[370] 2.20500 2.20500 2.20500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[379] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500\n[388] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[397] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[406] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[415] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.17000 2.17000 2.19625\n[424] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[433] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[442] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[451] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[460] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[469] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[478] 2.22000 2.22000 2.22000 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000\n[487] 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000 6.44000 6.45500 6.45500\n[496] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[505] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[514] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[523] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.13000 2.22000 2.22000\n[532] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[541] 1.97500 1.97500 2.09000 2.12000 2.14500 2.16375 2.17000 2.17000 2.17000\n[550] 2.17000 2.17000 2.17000 6.45500 6.42500 6.42500 6.42500 6.42500 6.41000\n[559] 6.42500 6.42500 6.42500 6.42500 6.42500 6.42500      NA      NA      NA\n[568]      NA      NA      NA 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[577] 1.93500 1.93500 1.93500 1.93500 1.93500 2.01875 2.03500 2.03500 2.03500\n[586] 2.03500 2.03500 2.03500      NA 2.03000 2.03000 2.03000 2.03000 2.00375\n[595] 1.99500 1.99500 1.99500 1.99500 1.99500 1.99500\n\n# $ is a \"special symbol\" in R, because it is an operator used for indexing\n# $ is technically an illegal column name, but we can still use it with ``\n# same goes for column names consisting of numbers or other symbols, etc.\n#\n# 5) col_types\n# default=NULL\n# if NULL R guesses data type from first 1000 rows\n# can also specify manually (but be careful)\n# see ?read_delim and scroll to col_types for details\n#\n# 6) skip\n# number of lines to skip\n# default=0\n# can be very useful with messy data files\n#\n# 7) n_max\n# maximum number of lines to read\n# default=Inf\n#\n#"
  },
  {
    "objectID": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "href": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "title": "Data Import",
    "section": "Read in .xls/.xlsx files",
    "text": "Read in .xls/.xlsx files\n.xls and .xlsx are files created in Microsoft Excel. There are separate functions read_xls() and read_xlsx(), but I find it’s best to use the wrapper function read_excel(). This will automatically call the correct function and avoid an error from accidentally mis-specifying the file type.\nSee below for what happens if we call the wrong function for the file type:\n\n# the try() function will try to run the code\n# see tryCatch() for more error handling \n# this code doesn't work because it tries to read the wrong file type\ntry(read_xls(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\")))\n\nError : \n  filepath: /Users/seanconway/Github/DACSS_601_W24/posts/_data/poultry_tidy.xlsx\n  libxls error: Unable to open file\n\n\nThe code below works just fine, however:\n\n# this code works \npoultry_from_excel &lt;- try(read_excel(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\"),\n                                     skip = 5,\n                                     col_names = c(\"prod\",\"year\",\"month\",\"price\"))) \npoultry_from_excel \n\n\n\n  \n\n\n\nLet’s take a look at this tibble:\n\n# examining our tibble\nhead(poultry_from_excel) # view the first several rows\n\n\n\n  \n\n\ncolnames(poultry_from_excel) # print column names\n\n[1] \"prod\"  \"year\"  \"month\" \"price\"\n\nglimpse(poultry_from_excel) # tidy little summary of it\n\nRows: 596\nColumns: 4\n$ prod  &lt;chr&gt; \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"…\n$ year  &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n$ month &lt;chr&gt; \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"Novemb…\n$ price &lt;dbl&gt; 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, …\n\n# the package::function() syntax is only necessary if the package isn't loaded\n\nFunction documentation:\n\n# to view function documentation\n?read_excel\n\n# optional arguments\n# 1) sheet=NULL\n# number of the sheet to read in\n# by default it reads the first sheet\n\n# 2) range=NULL\n# range of cells to read in\n# uses the cellranger package to work with specific cells in Excel files\n# for more, see the cellranger package\n# https://cran.r-project.org/web/packages/cellranger/index.html\n\n# 3) col_names=TRUE\n# how to get column names (works the same as read_delim())\n\n# 4) col_types=NULL\n# types of data in columns (works the same as read_delim())\n\n# 5) skip = 0\n# number of lines to skip (works the same as read_delim())\n\n# 6) n_max=Inf\n# max lines to read (works the same as read_delim())"
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-.rdata-files",
    "href": "posts/example-data_import.html#reading-in-.rdata-files",
    "title": "Data Import",
    "section": "Reading in .RData Files",
    "text": "Reading in .RData Files\nReading .RData is less commonly needed, but it’s still important to know about. .RData is a file type exclusively associated with R. It’s commonly used when someone has performed operations with data and saved the results to give to collaborators.\nWe can use the load() function to load R objects into our R environment from a file:\n\n# running the load() function on the data file name will load the objects into your R environment\nload(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\npoultry_tidy\n\n\n\n  \n\n\n# there's now a poultry_tidy object in our R environment\n\nNote that we do not assign the data file to an object. Rather, it comes in as an object based on whatever the previous user named it as. If we try to assign it as an object, the object will only have the name of the data file, rather than the data itself:\n\n# note that this operation shouldn't include any variable assignment\ntest_dat &lt;- load(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\ntest_dat # now it contains the object name, not the object itself\n\n[1] \"poultry_tidy\"\n\n\nYou can also save any number of R objects to a .RData file using the save() function:\n\na &lt;- rnorm(1000)\nb &lt;- matrix(runif(100),nrow=50,ncol=2)\nc &lt;- as_tibble(mtcars)\nsave(a,b,c,file=here(\"posts\",\"_data\",\"test_objs.RData\"))\n# there is now a test_objs.RData file in my working directory: \nlist.files(here(\"posts\",\"_data/\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"co2_data.txt\"                                                                                    \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"DS0001\"                                                                                          \n[15] \"eggs_tidy.csv\"                                                                                   \n[16] \"emissions.csv\"                                                                                   \n[17] \"End of the Semester Report Fall 2022.csv\"                                                        \n[18] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[19] \"FAOSTAT_country_groups.csv\"                                                                      \n[20] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[21] \"FAOSTAT_livestock.csv\"                                                                           \n[22] \"FedFundsRate.csv\"                                                                                \n[23] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[24] \"hotel_bookings.csv\"                                                                              \n[25] \"NBA_Player_Stats.csv\"                                                                            \n[26] \"online_retail.csv\"                                                                               \n[27] \"organiceggpoultry.xls\"                                                                           \n[28] \"poultry_tidy.csv\"                                                                                \n[29] \"poultry_tidy.RData\"                                                                              \n[30] \"poultry_tidy.xlsx\"                                                                               \n[31] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[32] \"railroad_2012_clean_county.csv\"                                                                  \n[33] \"sce-labor-chart-data-public.xlsx\"                                                                \n[34] \"snl_actors.csv\"                                                                                  \n[35] \"snl_casts.csv\"                                                                                   \n[36] \"snl_seasons.csv\"                                                                                 \n[37] \"starwars1.RData\"                                                                                 \n[38] \"StateCounty2012.xls\"                                                                             \n[39] \"test_objs.RData\"                                                                                 \n[40] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[41] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[42] \"wild_bird_data.xlsx\"                                                                             \n\n\nLet’s remove these objects from our R environment and re-load them from the file we saved:\n\n# remove objects from environment\nrm(list=c(\"a\",\"b\",\"c\"))\n\n# now they're back! (If you save them)\ntry(load(here(\"posts\",\"_data\",\"test_objs.RData\")))"
  },
  {
    "objectID": "posts/example-data_import.html#conclusion",
    "href": "posts/example-data_import.html#conclusion",
    "title": "Data Import",
    "section": "Conclusion",
    "text": "Conclusion\nYou now know a little bit about how to read in some common data types. Note that these aren’t the only types of data you’ll encounter, but they are by far the most common ones."
  },
  {
    "objectID": "tmp.html",
    "href": "tmp.html",
    "title": "Challenge 1",
    "section": "",
    "text": "1+1\n\n[1] 2\n\n\nThis is my challenge."
  },
  {
    "objectID": "posts/example-groupby_summarize.html",
    "href": "posts/example-groupby_summarize.html",
    "title": "group_by() & summarise()",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(dplyr)\nlibrary(here)\nlibrary(readr)"
  },
  {
    "objectID": "posts/example-groupby_summarize.html#overview",
    "href": "posts/example-groupby_summarize.html#overview",
    "title": "group_by() & summarise()",
    "section": "Overview",
    "text": "Overview\nToday, we’re going to read in the poultry_tidy data and use group_by(), mutate(), summarise() to perform simple operations. We will also discuss the use of the pipe (%&gt;%) as a way to streamline data operations."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#importing-the-data",
    "href": "posts/example-groupby_summarize.html#importing-the-data",
    "title": "group_by() & summarise()",
    "section": "Importing the data",
    "text": "Importing the data\n\npoultry &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry"
  },
  {
    "objectID": "posts/example-groupby_summarize.html#summarisesummarize",
    "href": "posts/example-groupby_summarize.html#summarisesummarize",
    "title": "group_by() & summarise()",
    "section": "Summarise/Summarize",
    "text": "Summarise/Summarize\nsummarise() is a function that allows you to perform multiple data summaries at once. It is one of several “workhorse” functions from the dplyr package, which we will use quite a bit this term. Note that you can also use summarize(), and it will work just the same.\nFor example, imagine we want to you use the mean() function to calculate the average price of poultry in our dataset. However, we also want to calculate the standard deviation (using sd()) to get a sense of the variability in our data. Standard deviation is a measure of how much the values in a variable differ from the average.\nIn Base R, we would need to do this in two separate lines.\n\nmean(poultry$Price_Dollar,na.rm=T) # there are some NA values we need to ignore\n\n[1] 3.390472\n\nsd(poultry$Price_Dollar,na.rm=T)\n\n[1] 1.731353\n\n\nUsing summarise(), we can calculate both of these at once, and without using the $ syntax. That is, dplyr uses something called “data masking” to make working with variables in a tibble/data frame easier (this is sort of an advanced topic that I’ll gloss over for now, but see this link if you would like to learn more).\n\nsummarise(poultry,\n          mean_price=mean(Price_Dollar,na.rm=T),\n          sd_price=sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nThis is handy because it creates a nice looking table for us. Notice that I was even allowed to give my “new” variables custom names. Without this, the column names will default to the code used to create them and it can look kind of ugly.\n\nsummarise(poultry,\n          mean(Price_Dollar,na.rm=T),\n          sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nI can use many different functions within summarise() .Really any function that allows me to distill a variable down to a single value, including median, variance, etc.\n\nsummarise(poultry,\n          mean_price=mean(Price_Dollar,na.rm=T),\n          median_price=median(Price_Dollar,na.rm=T),\n          var_price=var(Price_Dollar,na.rm=T),\n          sd_price=sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nHere we can see that the mean_price is higher than the median_price. You haven’t yet gotten to the stats tutorials, but this suggests that there are some relatively high priced products that are driving the mean price up. The median will be fairly robust to these types of values, so it’s a bit lower."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#mutate",
    "href": "posts/example-groupby_summarize.html#mutate",
    "title": "group_by() & summarise()",
    "section": "mutate()",
    "text": "mutate()\nIn addition to summarise(), there’s another workhorse function used for creating new columns: mutate().\nImagine we want to convert price from dollars to cents1. We can do so, using the mutate() function, which adds a new column to an existing data frame.2 Below we tell the mutate() function to create a new column, within the poultry data frame, called Price_Cents, which is computed as the price in dollars multiplied by 100.\n\nmutate(poultry, Price_Cents=Price_Dollar*100)\n\n\n\n  \n\n\n\nWe now see a new column, called Price_Cents, that is indeed Price_Dollar multiplied by 100.\nHowever, if I run the line below to take another look at the poultry, Price_Cents is gone.\n\npoultry\n\n\n\n  \n\n\n\nThis is because we only ran that line of code creating the column. We didn’t actually store it as an object3 in our environment. To do so, we need to use the &lt;- operator. &lt;- is also called the assignment operator, because it assigns R objects specific names. Generally speaking, &lt;- creates a new object in your environment. We are going to call our new tibble poultry_1\n\npoultry_1 &lt;- mutate(poultry, Price_Cents=Price_Dollar*100)\n\nTo take a look at this new data frame, we will just run the name as a separate line of code.\n\npoultry_1\n\n\n\n  \n\n\n\nNow we see Price_Cents stored as a column.\nWe’re going to move on to group_by() , where we’ll discuss creating groups within a dataframe. We won’t use poultry_1 any more, because Price_Cents is redundant with Price_Dollar, and cents is probably less informative tha dollar to most people. However, mutate() is a powerful tool (and an alternative to the base R $ syntax) for creating new variables in a data frame."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#group_by",
    "href": "posts/example-groupby_summarize.html#group_by",
    "title": "group_by() & summarise()",
    "section": "group_by()",
    "text": "group_by()\nOften we don’t just care about a single numerical summary of a variable - rather, we want to know how that variable changes (or remains constant) across another, categorical variable. For example, we may want to know how salary changes by gender or ethnicity, or how carbon emissions change by state.\ngroup_by() allows us to create “groups” in the data based on one or more variables (referred to as grouping variables). We can then use summarise() to calculate separate summary statistics for each group.\nBelow, I use group_by() to tell R that I want to take the poultry dataset and group by the column Product. Then, I pass this grouped data frame to summarise(), where I again compute the mean price.\n\nsummarise(group_by(poultry, Product),\n          mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nIt appears that boneless skinless breasts are by far the most expensive poultry product within this dataset."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#pipes",
    "href": "posts/example-groupby_summarize.html#pipes",
    "title": "group_by() & summarise()",
    "section": "Pipes",
    "text": "Pipes\nYou might have noticed that the above operation using group_by() and summarise() looked a little clunky. We wrapped the group_by code within the summarise function, and it was a bit hard to read.\nThere’s a way around this, however! We can use the pipe operator (%&gt;%) to streamline our operations. The pipe allows us to pass a take a tibble/data frame and perform multiple intermediate operations on it, passing the modified data frame through on each step.\n\npoultry %&gt;%\n  group_by(Product) %&gt;%\n  summarise(mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nVoila!\nLet’s break that down a bit. First, I entered poultry, the name of the tibble I was working with. I then used the pipe, to pass poultry through to the group_by() function, where I created groups based on the column Product. If I run just these first two lines, we can see that poultry looks the same, but R now tells us that there are 5 groups present, based on the variable Product.\n\npoultry %&gt;%\n  group_by(Product)\n\n\n\n  \n\n\n\nFinally, after we group by Product, we can then pass this through to the summarise function using a pipe, where we compute the mean price in the same way as before.\n\npoultry %&gt;%\n  group_by(Product) %&gt;%\n  summarise(mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nThe pipe is a powerful tool that we will use quite a bit in this class. It does take some getting used to, and I don’t expect it to click right away. With practice, however, you will become proficient at using it."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#conclusion",
    "href": "posts/example-groupby_summarize.html#conclusion",
    "title": "group_by() & summarise()",
    "section": "Conclusion",
    "text": "Conclusion\nNow you’ve learned a little bit about how to summarize data, and in particular how to use group_by() to summarize it based on a grouping variable. We also learned how to create new columns within an existing data frame."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#footnotes",
    "href": "posts/example-groupby_summarize.html#footnotes",
    "title": "group_by() & summarise()",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m not actually sure why you would want to do so. This is just an example of how to use mutate().↩︎\nmutate() can also modify an existing column, but we won’t be using it for that here.↩︎\nAn object is a generic term for any variable in your R environment.↩︎"
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xls ⭐\nFAOstat*.csv or birds.csv ⭐⭐⭐\nhotel_bookings.csv ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a brief, high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set. Describe what you find."
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readxl)\nlibrary(here)\n\n\nhere() starts at /Users/seanconway/Github/DACSS_601_W24\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#challenge-overview",
    "href": "posts/challenge1_solutions.html#challenge-overview",
    "title": "Challenge 1 Solution",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#read-in-the-data",
    "href": "posts/challenge1_solutions.html#read-in-the-data",
    "title": "Challenge 1 Solution",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\nYou should have already downloaded the datasets from Google Classroom and stored them in a common directory on your computer.\nIn this challenge, as in all subsequent challenges, the number of stars corresponds to the difficulty of the dataset. You are only required to do the challenge on one dataset, though you are welcome to do it with multiple datasets.\nIn general, I encourage you to “challenge” yourself by trying to work with a dataset above your experience.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat\\*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation.\n\nRailroad ⭐FAOSTAT / birds⭐⭐Wild Birds ⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad &lt;- here(\"posts\",\"_data\",\"railroad_2012_clean_county.csv\") %&gt;%\n  read_csv()\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the file, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad %&gt;%\n  select(state) %&gt;%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%&gt;%\n  select(state)%&gt;%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estimates of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\nWe’re also lumping in the birds.csv dataset here, because it comes from the same source.\n\nRead the Data\n\n\nCode\nbirds &lt;- here(\"posts\",\"_data\",\"birds.csv\") %&gt;%\n  read_csv()\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens &lt;- here(\"posts\",\"_data\",\"FAOSTAT_egg_chicken.csv\") %&gt;%\n  read_csv()\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncattle &lt;- here(\"posts\",\"_data\",\"FAOSTAT_cattle_dairy.csv\") %&gt;%\n  read_csv()\n\n\nRows: 36449 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncountry &lt;- here(\"posts\",\"_data\",\"FAOSTAT_country_groups.csv\") %&gt;%\n  read_csv()\n\n\nRows: 1943 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Country Group, Country, M49 Code, ISO2 Code, ISO3 Code\ndbl (2): Country Group Code, Country Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nlivestock &lt;- here(\"posts\",\"_data\",\"FAOSTAT_livestock.csv\") %&gt;%\n  read_csv()\n\n\nRows: 82116 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nCode\ncattle\n\n\n\n\n  \n\n\n\nCode\ncountry\n\n\n\n\n  \n\n\n\nCode\nlivestock\n\n\n\n\n  \n\n\n\n\n\nCode\nglimpse(birds)\n\n\nRows: 30,977\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA…\n$ Domain             &lt;chr&gt; \"Live Animals\", \"Live Animals\", \"Live Animals\", \"Li…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5112, 5112, 5112, 5112, 5112, 5112, 5112, 5112, 511…\n$ Element            &lt;chr&gt; \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"…\n$ `Item Code`        &lt;dbl&gt; 1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057, 105…\n$ Item               &lt;chr&gt; \"Chickens\", \"Chickens\", \"Chickens\", \"Chickens\", \"Ch…\n$ `Year Code`        &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Year               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Unit               &lt;chr&gt; \"1000 Head\", \"1000 Head\", \"1000 Head\", \"1000 Head\",…\n$ Value              &lt;dbl&gt; 4700, 4900, 5000, 5300, 5500, 5800, 6600, 6290, 630…\n$ Flag               &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", NA, \"F\", \"F\", \"F…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"FAO estimate\", \"FAO estimate\", \"FA…\n\n\nCode\nglimpse(chickens)\n\n\nRows: 38,170\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL…\n$ Domain             &lt;chr&gt; \"Livestock Primary\", \"Livestock Primary\", \"Livestoc…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5313, 5410, 5510, 5313, 5410, 5510, 5313, 5410, 551…\n$ Element            &lt;chr&gt; \"Laying\", \"Yield\", \"Production\", \"Laying\", \"Yield\",…\n$ `Item Code`        &lt;dbl&gt; 1062, 1062, 1062, 1062, 1062, 1062, 1062, 1062, 106…\n$ Item               &lt;chr&gt; \"Eggs, hen, in shell\", \"Eggs, hen, in shell\", \"Eggs…\n$ `Year Code`        &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Year               &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Unit               &lt;chr&gt; \"1000 Head\", \"100mg/An\", \"tonnes\", \"1000 Head\", \"10…\n$ Value              &lt;dbl&gt; 4000, 25000, 10000, 4400, 25000, 11000, 4600, 25000…\n$ Flag               &lt;chr&gt; \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\"…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"Calculated data\", \"FAO estimate\", …\n\n\nCode\nglimpse(cattle)\n\n\nRows: 36,449\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL…\n$ Domain             &lt;chr&gt; \"Livestock Primary\", \"Livestock Primary\", \"Livestoc…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5318, 5420, 5510, 5318, 5420, 5510, 5318, 5420, 551…\n$ Element            &lt;chr&gt; \"Milk Animals\", \"Yield\", \"Production\", \"Milk Animal…\n$ `Item Code`        &lt;dbl&gt; 882, 882, 882, 882, 882, 882, 882, 882, 882, 882, 8…\n$ Item               &lt;chr&gt; \"Milk, whole fresh cow\", \"Milk, whole fresh cow\", \"…\n$ `Year Code`        &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Year               &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Unit               &lt;chr&gt; \"Head\", \"hg/An\", \"tonnes\", \"Head\", \"hg/An\", \"tonnes…\n$ Value              &lt;dbl&gt; 700000, 5000, 350000, 700000, 5000, 350000, 780000,…\n$ Flag               &lt;chr&gt; \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\"…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"Calculated data\", \"FAO estimate\", …\n\n\nCode\nglimpse(country)\n\n\nRows: 1,943\nColumns: 7\n$ `Country Group Code` &lt;dbl&gt; 5100, 5100, 5100, 5100, 5100, 5100, 5100, 5100, 5…\n$ `Country Group`      &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\",…\n$ `Country Code`       &lt;dbl&gt; 4, 7, 53, 20, 233, 29, 35, 32, 37, 39, 24, 45, 46…\n$ Country              &lt;chr&gt; \"Algeria\", \"Angola\", \"Benin\", \"Botswana\", \"Burkin…\n$ `M49 Code`           &lt;chr&gt; \"012\", \"024\", \"204\", \"072\", \"854\", \"108\", \"132\", …\n$ `ISO2 Code`          &lt;chr&gt; \"DZ\", \"AO\", \"BJ\", \"BW\", \"BF\", \"BI\", \"CV\", \"CM\", \"…\n$ `ISO3 Code`          &lt;chr&gt; \"DZA\", \"AGO\", \"BEN\", \"BWA\", \"BFA\", \"BDI\", \"CPV\", …\n\n\nCode\nglimpse(livestock)\n\n\nRows: 82,116\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA…\n$ Domain             &lt;chr&gt; \"Live Animals\", \"Live Animals\", \"Live Animals\", \"Li…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5111, 5111, 5111, 5111, 5111, 5111, 5111, 5111, 511…\n$ Element            &lt;chr&gt; \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"…\n$ `Item Code`        &lt;dbl&gt; 1107, 1107, 1107, 1107, 1107, 1107, 1107, 1107, 110…\n$ Item               &lt;chr&gt; \"Asses\", \"Asses\", \"Asses\", \"Asses\", \"Asses\", \"Asses…\n$ `Year Code`        &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Year               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Unit               &lt;chr&gt; \"Head\", \"Head\", \"Head\", \"Head\", \"Head\", \"Head\", \"He…\n$ Value              &lt;dbl&gt; 1300000, 851850, 1001112, 1150000, 1300000, 1200000…\n$ Flag               &lt;chr&gt; NA, NA, NA, \"F\", NA, NA, NA, NA, NA, NA, NA, NA, \"F…\n$ `Flag Description` &lt;chr&gt; \"Official data\", \"Official data\", \"Official data\", …\n\n\nThere’s clearly a lot going on with these data, but using the glimpse() function, as well as scrolling through the data, allows us to get more of a handle on the datasets. The columns containing Code appear to be redundant with another column, so we can likely ignore them for now.\nFor now, we can focus on birds, cattle, chickens, and livestock.\nIn all 4 of these datasets, the Area column indicates the location of the agricultural product. Element indicates the type of product, Item indicates the animal. Year indicates the year of estimate. Unit indicates the unit of measurement.\nThese data are “long” or “tidy”. Don’t worry if you don’t know what this means, yet - we haven’t gotten there in the course. Each row appears to compromise a case - a single measurement. A single row will contain a measurement of an agricultural product from a single year and in a single country.\nThe country dataset is less interesting. It only contains the codes to match Country and Country groups. We may need to go to the FAOSTAT website to figure this out more. For now, we’ll move on to the next dataset.\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in than an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;%\n  read_excel()\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip argument from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;%\n  read_excel(skip = 1)\nwildbirds \n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;% \n  read_excel(skip = 2,col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\nThe data are pretty straightforward to interpret.\n\n\nCode\nglimpse(wildbirds)\n\n\nRows: 146\nColumns: 2\n$ weight   &lt;dbl&gt; 5.458872, 7.764568, 8.638587, 10.689735, 7.417226, 9.116935, …\n$ pop_size &lt;dbl&gt; 532194.3951, 3165107.4454, 2592996.8678, 3524193.2266, 389806…\n\n\nEach row is a single case, with measurements of weight and population size for (presumably) a single species of bird.\nWe may need to take a look at the publication if we want to figure out the species’ name. This is above and beyond this challenge, so we will move on.\n## Railroad (xls) ⭐⭐⭐⭐\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nhere(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(skip=3)\n\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nhere(\"posts\",\"_data\",\"StateCounty2012.xls\")  %&gt;%\n  read_excel(skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%&gt;%\n  select(-contains(\"delete\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(skip = 4,col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%&gt;%\n  select(!contains(\"delete\"))%&gt;%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad &lt;- head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nThe range approach\nWe can manually specify the range of cells we want to read in using the range argument. To do so, you’ll need to open the file up in Excel (or a similar program) and figure this out on your own.\n\n\nCode\nrailroad_new &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(range = \"B4:F2990\", col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\")) %&gt;%\n  select(!contains(\"delete\"))%&gt;%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\nCode\nrailroad_new\n\n\n\n\n  \n\n\n\nCode\ntail(railroad_new,10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%&gt;%\n  select(State)%&gt;%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%&gt;%\n  select(State)%&gt;%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  }
]
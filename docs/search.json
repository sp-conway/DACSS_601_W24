[
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\nYou should have already downloaded the datasets from Google Classroom and stored them in a common directory on your computer.\nIn this challenge, as in all subsequent challenges, the number of stars corresponds to the difficulty of the dataset. You are only required to do the challenge on one dataset, though you are welcome to do it with multiple datasets.\nIn general, I encourage you to “challenge” yourself by trying to work with a dataset above your experience.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat\\*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 30, 2023\n\n\nChallenge 2 Solutions\n\n\nSean Conway\n\n\n\n\nDec 30, 2023\n\n\nChallenge 3 Solutions\n\n\nSean Conway\n\n\n\n\nDec 27, 2023\n\n\nChallenge 6 Instructions\n\n\nSean Conway\n\n\n\n\nDec 27, 2023\n\n\nChallenge 5 Instructions\n\n\nSean Conway\n\n\n\n\nDec 27, 2023\n\n\nData analysis pipeline\n\n\nSean Conway\n\n\n\n\nDec 27, 2023\n\n\nChallenge 4 Instructions\n\n\nSean Conway\n\n\n\n\nDec 24, 2023\n\n\nChallenge 3 Instructions\n\n\nSean Conway\n\n\n\n\nDec 24, 2023\n\n\npivoting/tidy data\n\n\nSean Conway\n\n\n\n\nDec 23, 2023\n\n\nChallenge 1 Solution\n\n\nSean Conway\n\n\n\n\nDec 21, 2023\n\n\nChallenge 2 Instructions\n\n\nSean Conway\n\n\n\n\nDec 20, 2023\n\n\ngroup_by() & summarise()\n\n\nSean Conway\n\n\n\n\nDec 18, 2023\n\n\nData Import\n\n\nSean Conway\n\n\n\n\nDec 15, 2023\n\n\nChallenge 1 Instructions\n\n\nSean Conway\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog contains challenges, solutions, and demonstration scripts for DACSS 601."
  },
  {
    "objectID": "posts/example-data_import.html",
    "href": "posts/example-data_import.html",
    "title": "Data Import",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(dplyr)\nlibrary(here)\nlibrary(readxl)\nlibrary(readr)"
  },
  {
    "objectID": "posts/example-data_import.html#overview",
    "href": "posts/example-data_import.html#overview",
    "title": "Data Import",
    "section": "Overview",
    "text": "Overview\nToday, we’re going to read in three versions of the poultry_tidy data. These data are available on the Google Classroom.\nWe will specifically read in 3 data files:\n- poultry_tidy.csv\n- poultry_tidy.xlsx\n- poultry_tidy.RData\nThese are the “clean” versions of the raw data files.\nTo run this file, all 3 datasets should be in the same directory on your computer.\nOn my computer, I have all datasets stored in a folder named _data.\nI also use the here package to manage relative directories."
  },
  {
    "objectID": "posts/example-data_import.html#getting-started",
    "href": "posts/example-data_import.html#getting-started",
    "title": "Data Import",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin, we need to load two packages: readr and readxl, which contain very useful functions for reading in data to `R.\n\nlibrary(readxl)\n\nIf you’re unsure whether or not you have these packages installed, you can run the following command:\n\ninstalled.packages()\n\nWe’re now ready to get started reading in actual datasets."
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-delimited-text-files",
    "href": "posts/example-data_import.html#reading-in-delimited-text-files",
    "title": "Data Import",
    "section": "Reading in delimited text files",
    "text": "Reading in delimited text files\n.csv is a common type of delimited text file. .csv stands for comma-separated value. This means that commas separate cells from one another.\nR has a base read.csv() function. However, it comes with a couple of downsides - namely that it imports data as a dataframe rather than a tibble. So we will be using the function read_csv() from the readr package. In addition to importing data as a tibble, it also does a much better job guessing data types.\nread_csv() is essentially a wrapper function (a function that calls another function) around the more general read_delim() function. Also see read_tsv() for tab-separated values.\n\n?read_delim\n\nLet’s look at the data files available for us to read in:\n\nlist.files(here(\"posts\",\"_data\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"co2_data.txt\"                                                                                    \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"DS0001\"                                                                                          \n[15] \"eggs_tidy.csv\"                                                                                   \n[16] \"emissions.csv\"                                                                                   \n[17] \"End of the Semester Report Fall 2022.csv\"                                                        \n[18] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[19] \"FAOSTAT_country_groups.csv\"                                                                      \n[20] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[21] \"FAOSTAT_livestock.csv\"                                                                           \n[22] \"FedFundsRate.csv\"                                                                                \n[23] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[24] \"hotel_bookings.csv\"                                                                              \n[25] \"NBA_Player_Stats.csv\"                                                                            \n[26] \"online_retail.csv\"                                                                               \n[27] \"organiceggpoultry.xls\"                                                                           \n[28] \"poultry_tidy.csv\"                                                                                \n[29] \"poultry_tidy.RData\"                                                                              \n[30] \"poultry_tidy.xlsx\"                                                                               \n[31] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[32] \"railroad_2012_clean_county.csv\"                                                                  \n[33] \"sce-labor-chart-data-public.xlsx\"                                                                \n[34] \"snl_actors.csv\"                                                                                  \n[35] \"snl_casts.csv\"                                                                                   \n[36] \"snl_seasons.csv\"                                                                                 \n[37] \"starwars1.RData\"                                                                                 \n[38] \"StateCounty2012.xls\"                                                                             \n[39] \"test_objs.RData\"                                                                                 \n[40] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[41] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[42] \"wild_bird_data.xlsx\"                                                                             \n\n\nThere’s a lot of data files there, but we are going to import the poultry_tidy.csv file. Doing so is very simple using read_csv():\n\npoultry_from_csv &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our dataset (to view the tibble, running the name of the object will print it to the console):\n\npoultry_from_csv\n\n\n\n  \n\n\nhead(poultry_from_csv)\n\n\n\n  \n\n\n\nIt worked great! The data is all there. To inspect the data types for each of the four columns in poultry_from_csv, we can use spec() or typeof():\n\npoultry_from_csv &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\n\nspec(poultry_from_csv) # use the spec() function to check the data type for your columns\n\ncols(\n  Product = col_character(),\n  Year = col_double(),\n  Month = col_character(),\n  Price_Dollar = col_double()\n)\n\n# can also use typeof() function on individual columns\ntypeof(poultry_from_csv$Product)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Year)\n\n[1] \"double\"\n\ntypeof(poultry_from_csv$Month)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Price_Dollar)\n\n[1] \"double\"\n\n\nSee this R section below for some more info on read_delim():\n\n# read_delim() has a number of optional arguments\nargs(read_delim)\n\nfunction (file, delim = NULL, quote = \"\\\"\", escape_backslash = FALSE, \n    escape_double = TRUE, col_names = TRUE, col_types = NULL, \n    col_select = NULL, id = NULL, locale = default_locale(), \n    na = c(\"\", \"NA\"), quoted_na = TRUE, comment = \"\", trim_ws = FALSE, \n    skip = 0, n_max = Inf, guess_max = min(1000, n_max), name_repair = \"unique\", \n    num_threads = readr_threads(), progress = show_progress(), \n    show_col_types = should_show_types(), skip_empty_rows = TRUE, \n    lazy = should_read_lazy()) \nNULL\n\n# there's too many to list here, so we will just go over a few\n# run ?read_delim() to learn more\n# 1) delim - text delimiter.\n# default is NULL and read_delim() guesses delimiter\n#\n# 2) quote - symbol telling R when to quote a string\n# default is \"\\\"\"\n# below comes from R documentation on quotes\n# https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html\n# identical() is a function that returns TRUE if two objects are equal\nidentical(1+4, 3+2)\n\n[1] TRUE\n\nidentical('\"It\\'s alive!\", he screamed.',\n          \"\\\"It's alive!\\\", he screamed.\") # same\n\n[1] TRUE\n\n#\n# 3) escape_backlash\n# use backlash to escape special characters?\n# default = FALSE\n#\n# 4) col_names\n# can be TRUE (default), meaning that R reads in the first row of values as column names\n# can FALSE - R creates column names (x1 x2 etc)\n# OR can be a character vector of custom column names\npoultry_custom_cols &lt;- read_csv(\"_data/poultry_tidy.csv\",\n                                col_names = c(\"prod\",\"yr\",\"mo\",\"$\"),\n                                skip = 1) # need this to skip the file's column names\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): prod, mo\ndbl (2): yr, $\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry_custom_cols\n\n\n\n  \n\n\npoultry_custom_cols$`$` # note the backticks around the $ sign\n\n  [1] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [10] 2.38500 2.38500 2.38500 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750\n [19] 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750 3.90500 3.90500 3.90500\n [28] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [37] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n [46] 2.03500 2.03500 2.03500 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250\n [55] 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250 2.35000 2.38500 2.38500\n [64] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [73] 6.37500 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000\n [82] 7.00000 7.03750 7.03750 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [91] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[100] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[109] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.16250 2.16250\n[118] 2.16250 2.16250 2.16250 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000\n[127] 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000 6.37500 6.37500 6.37500\n[136] 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500\n[145] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[154] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[163] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.15000 2.15000 2.15000\n[172] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000\n[181] 2.48000 2.48000 2.48000 2.41500 2.35000 2.35000 2.41500 2.35000 2.35000\n[190] 2.35000 2.35000 2.35000 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[199] 6.45500 6.42300 6.37500 6.37500 6.37500 6.37500 3.90500 3.90500 3.90500\n[208] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[217] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[226] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[235] 2.22000 2.19200 2.15000 2.15000 2.15000 2.15000 2.48000 2.48000 2.48000\n[244] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000\n[253] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[262] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[271] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[280] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[289] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[298] 2.22000 2.22000 2.22000 2.20500 2.20500 2.20500 2.20500 2.20500 2.48000\n[307] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 6.45500 6.45500 6.45500\n[316] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[325] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[334] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[343] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000\n[352] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[361] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[370] 2.20500 2.20500 2.20500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[379] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500\n[388] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[397] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[406] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[415] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.17000 2.17000 2.19625\n[424] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[433] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[442] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[451] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[460] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[469] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[478] 2.22000 2.22000 2.22000 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000\n[487] 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000 6.44000 6.45500 6.45500\n[496] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[505] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[514] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[523] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.13000 2.22000 2.22000\n[532] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[541] 1.97500 1.97500 2.09000 2.12000 2.14500 2.16375 2.17000 2.17000 2.17000\n[550] 2.17000 2.17000 2.17000 6.45500 6.42500 6.42500 6.42500 6.42500 6.41000\n[559] 6.42500 6.42500 6.42500 6.42500 6.42500 6.42500      NA      NA      NA\n[568]      NA      NA      NA 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[577] 1.93500 1.93500 1.93500 1.93500 1.93500 2.01875 2.03500 2.03500 2.03500\n[586] 2.03500 2.03500 2.03500      NA 2.03000 2.03000 2.03000 2.03000 2.00375\n[595] 1.99500 1.99500 1.99500 1.99500 1.99500 1.99500\n\n# $ is a \"special symbol\" in R, because it is an operator used for indexing\n# $ is technically an illegal column name, but we can still use it with ``\n# same goes for column names consisting of numbers or other symbols, etc.\n#\n# 5) col_types\n# default=NULL\n# if NULL R guesses data type from first 1000 rows\n# can also specify manually (but be careful)\n# see ?read_delim and scroll to col_types for details\n#\n# 6) skip\n# number of lines to skip\n# default=0\n# can be very useful with messy data files\n#\n# 7) n_max\n# maximum number of lines to read\n# default=Inf\n#\n#"
  },
  {
    "objectID": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "href": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "title": "Data Import",
    "section": "Read in .xls/.xlsx files",
    "text": "Read in .xls/.xlsx files\n.xls and .xlsx are files created in Microsoft Excel. There are separate functions read_xls() and read_xlsx(), but I find it’s best to use the wrapper function read_excel(). This will automatically call the correct function and avoid an error from accidentally mis-specifying the file type.\nSee below for what happens if we call the wrong function for the file type:\n\n# the try() function will try to run the code\n# see tryCatch() for more error handling \n# this code doesn't work because it tries to read the wrong file type\ntry(read_xls(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\")))\n\nError : \n  filepath: /Users/seanconway/Github/DACSS_601_W24/posts/_data/poultry_tidy.xlsx\n  libxls error: Unable to open file\n\n\nThe code below works just fine, however:\n\n# this code works \npoultry_from_excel &lt;- try(read_excel(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\"),\n                                     skip = 5,\n                                     col_names = c(\"prod\",\"year\",\"month\",\"price\"))) \npoultry_from_excel \n\n\n\n  \n\n\n\nLet’s take a look at this tibble:\n\n# examining our tibble\nhead(poultry_from_excel) # view the first several rows\n\n\n\n  \n\n\ncolnames(poultry_from_excel) # print column names\n\n[1] \"prod\"  \"year\"  \"month\" \"price\"\n\nglimpse(poultry_from_excel) # tidy little summary of it\n\nRows: 596\nColumns: 4\n$ prod  &lt;chr&gt; \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"…\n$ year  &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n$ month &lt;chr&gt; \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"Novemb…\n$ price &lt;dbl&gt; 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, …\n\n# the package::function() syntax is only necessary if the package isn't loaded\n\nFunction documentation:\n\n# to view function documentation\n?read_excel\n\n# optional arguments\n# 1) sheet=NULL\n# number of the sheet to read in\n# by default it reads the first sheet\n\n# 2) range=NULL\n# range of cells to read in\n# uses the cellranger package to work with specific cells in Excel files\n# for more, see the cellranger package\n# https://cran.r-project.org/web/packages/cellranger/index.html\n\n# 3) col_names=TRUE\n# how to get column names (works the same as read_delim())\n\n# 4) col_types=NULL\n# types of data in columns (works the same as read_delim())\n\n# 5) skip = 0\n# number of lines to skip (works the same as read_delim())\n\n# 6) n_max=Inf\n# max lines to read (works the same as read_delim())"
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-.rdata-files",
    "href": "posts/example-data_import.html#reading-in-.rdata-files",
    "title": "Data Import",
    "section": "Reading in .RData Files",
    "text": "Reading in .RData Files\nReading .RData is less commonly needed, but it’s still important to know about. .RData is a file type exclusively associated with R. It’s commonly used when someone has performed operations with data and saved the results to give to collaborators.\nWe can use the load() function to load R objects into our R environment from a file:\n\n# running the load() function on the data file name will load the objects into your R environment\nload(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\npoultry_tidy\n\n\n\n  \n\n\n# there's now a poultry_tidy object in our R environment\n\nNote that we do not assign the data file to an object. Rather, it comes in as an object based on whatever the previous user named it as. If we try to assign it as an object, the object will only have the name of the data file, rather than the data itself:\n\n# note that this operation shouldn't include any variable assignment\ntest_dat &lt;- load(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\ntest_dat # now it contains the object name, not the object itself\n\n[1] \"poultry_tidy\"\n\n\nYou can also save any number of R objects to a .RData file using the save() function:\n\na &lt;- rnorm(1000)\nb &lt;- matrix(runif(100),nrow=50,ncol=2)\nc &lt;- as_tibble(mtcars)\nsave(a,b,c,file=here(\"posts\",\"_data\",\"test_objs.RData\"))\n# there is now a test_objs.RData file in my working directory: \nlist.files(here(\"posts\",\"_data/\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"co2_data.txt\"                                                                                    \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"DS0001\"                                                                                          \n[15] \"eggs_tidy.csv\"                                                                                   \n[16] \"emissions.csv\"                                                                                   \n[17] \"End of the Semester Report Fall 2022.csv\"                                                        \n[18] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[19] \"FAOSTAT_country_groups.csv\"                                                                      \n[20] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[21] \"FAOSTAT_livestock.csv\"                                                                           \n[22] \"FedFundsRate.csv\"                                                                                \n[23] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[24] \"hotel_bookings.csv\"                                                                              \n[25] \"NBA_Player_Stats.csv\"                                                                            \n[26] \"online_retail.csv\"                                                                               \n[27] \"organiceggpoultry.xls\"                                                                           \n[28] \"poultry_tidy.csv\"                                                                                \n[29] \"poultry_tidy.RData\"                                                                              \n[30] \"poultry_tidy.xlsx\"                                                                               \n[31] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[32] \"railroad_2012_clean_county.csv\"                                                                  \n[33] \"sce-labor-chart-data-public.xlsx\"                                                                \n[34] \"snl_actors.csv\"                                                                                  \n[35] \"snl_casts.csv\"                                                                                   \n[36] \"snl_seasons.csv\"                                                                                 \n[37] \"starwars1.RData\"                                                                                 \n[38] \"StateCounty2012.xls\"                                                                             \n[39] \"test_objs.RData\"                                                                                 \n[40] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[41] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[42] \"wild_bird_data.xlsx\"                                                                             \n\n\nLet’s remove these objects from our R environment and re-load them from the file we saved:\n\n# remove objects from environment\nrm(list=c(\"a\",\"b\",\"c\"))\n\n# now they're back! (If you save them)\ntry(load(here(\"posts\",\"_data\",\"test_objs.RData\")))"
  },
  {
    "objectID": "posts/example-data_import.html#conclusion",
    "href": "posts/example-data_import.html#conclusion",
    "title": "Data Import",
    "section": "Conclusion",
    "text": "Conclusion\nYou now know a little bit about how to read in some common data types. Note that these aren’t the only types of data you’ll encounter, but they are by far the most common ones."
  },
  {
    "objectID": "tmp.html",
    "href": "tmp.html",
    "title": "Challenge 1",
    "section": "",
    "text": "1+1\n\n[1] 2\n\n\nThis is my challenge."
  },
  {
    "objectID": "posts/example-groupby_summarize.html",
    "href": "posts/example-groupby_summarize.html",
    "title": "group_by() & summarise()",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(dplyr)\nlibrary(here)\nlibrary(readr)"
  },
  {
    "objectID": "posts/example-groupby_summarize.html#overview",
    "href": "posts/example-groupby_summarize.html#overview",
    "title": "group_by() & summarise()",
    "section": "Overview",
    "text": "Overview\nToday, we’re going to read in the poultry_tidy data and use group_by(), mutate(), summarise() to perform simple operations. We will also discuss the use of the pipe (%&gt;%) as a way to streamline data operations."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#importing-the-data",
    "href": "posts/example-groupby_summarize.html#importing-the-data",
    "title": "group_by() & summarise()",
    "section": "Importing the data",
    "text": "Importing the data\n\npoultry &lt;- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry"
  },
  {
    "objectID": "posts/example-groupby_summarize.html#summarisesummarize",
    "href": "posts/example-groupby_summarize.html#summarisesummarize",
    "title": "group_by() & summarise()",
    "section": "Summarise/Summarize",
    "text": "Summarise/Summarize\nsummarise() is a function that allows you to perform multiple data summaries at once. It is one of several “workhorse” functions from the dplyr package, which we will use quite a bit this term. Note that you can also use summarize(), and it will work just the same.\nFor example, imagine we want to you use the mean() function to calculate the average price of poultry in our dataset. However, we also want to calculate the standard deviation (using sd()) to get a sense of the variability in our data. Standard deviation is a measure of how much the values in a variable differ from the average.\nIn Base R, we would need to do this in two separate lines.\n\nmean(poultry$Price_Dollar,na.rm=T) # there are some NA values we need to ignore\n\n[1] 3.390472\n\nsd(poultry$Price_Dollar,na.rm=T)\n\n[1] 1.731353\n\n\nUsing summarise(), we can calculate both of these at once, and without using the $ syntax. That is, dplyr uses something called “data masking” to make working with variables in a tibble/data frame easier (this is sort of an advanced topic that I’ll gloss over for now, but see this link if you would like to learn more).\n\nsummarise(poultry,\n          mean_price=mean(Price_Dollar,na.rm=T),\n          sd_price=sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nThis is handy because it creates a nice looking table for us. Notice that I was even allowed to give my “new” variables custom names. Without this, the column names will default to the code used to create them and it can look kind of ugly.\n\nsummarise(poultry,\n          mean(Price_Dollar,na.rm=T),\n          sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nI can use many different functions within summarise() .Really any function that allows me to distill a variable down to a single value, including median, variance, etc.\n\nsummarise(poultry,\n          mean_price=mean(Price_Dollar,na.rm=T),\n          median_price=median(Price_Dollar,na.rm=T),\n          var_price=var(Price_Dollar,na.rm=T),\n          sd_price=sd(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nHere we can see that the mean_price is higher than the median_price. You haven’t yet gotten to the stats tutorials, but this suggests that there are some relatively high priced products that are driving the mean price up. The median will be fairly robust to these types of values, so it’s a bit lower."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#mutate",
    "href": "posts/example-groupby_summarize.html#mutate",
    "title": "group_by() & summarise()",
    "section": "mutate()",
    "text": "mutate()\nIn addition to summarise(), there’s another workhorse function used for creating new columns: mutate().\nImagine we want to convert price from dollars to cents1. We can do so, using the mutate() function, which adds a new column to an existing data frame.2 Below we tell the mutate() function to create a new column, within the poultry data frame, called Price_Cents, which is computed as the price in dollars multiplied by 100.\n\nmutate(poultry, Price_Cents=Price_Dollar*100)\n\n\n\n  \n\n\n\nWe now see a new column, called Price_Cents, that is indeed Price_Dollar multiplied by 100.\nHowever, if I run the line below to take another look at the poultry, Price_Cents is gone.\n\npoultry\n\n\n\n  \n\n\n\nThis is because we only ran that line of code creating the column. We didn’t actually store it as an object3 in our environment. To do so, we need to use the &lt;- operator. &lt;- is also called the assignment operator, because it assigns R objects specific names. Generally speaking, &lt;- creates a new object in your environment. We are going to call our new tibble poultry_1\n\npoultry_1 &lt;- mutate(poultry, Price_Cents=Price_Dollar*100)\n\nTo take a look at this new data frame, we will just run the name as a separate line of code.\n\npoultry_1\n\n\n\n  \n\n\n\nNow we see Price_Cents stored as a column.\nWe’re going to move on to group_by() , where we’ll discuss creating groups within a dataframe. We won’t use poultry_1 any more, because Price_Cents is redundant with Price_Dollar, and cents is probably less informative tha dollar to most people. However, mutate() is a powerful tool (and an alternative to the base R $ syntax) for creating new variables in a data frame."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#group_by",
    "href": "posts/example-groupby_summarize.html#group_by",
    "title": "group_by() & summarise()",
    "section": "group_by()",
    "text": "group_by()\nOften we don’t just care about a single numerical summary of a variable - rather, we want to know how that variable changes (or remains constant) across another, categorical variable. For example, we may want to know how salary changes by gender or ethnicity, or how carbon emissions change by state.\ngroup_by() allows us to create “groups” in the data based on one or more variables (referred to as grouping variables). We can then use summarise() to calculate separate summary statistics for each group.\nBelow, I use group_by() to tell R that I want to take the poultry dataset and group by the column Product. Then, I pass this grouped data frame to summarise(), where I again compute the mean price.\n\nsummarise(group_by(poultry, Product),\n          mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nIt appears that boneless skinless breasts are by far the most expensive poultry product within this dataset."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#pipes",
    "href": "posts/example-groupby_summarize.html#pipes",
    "title": "group_by() & summarise()",
    "section": "Pipes",
    "text": "Pipes\nYou might have noticed that the above operation using group_by() and summarise() looked a little clunky. We wrapped the group_by code within the summarise function, and it was a bit hard to read.\nThere’s a way around this, however! We can use the pipe operator (%&gt;%) to streamline our operations. The pipe allows us to pass a take a tibble/data frame and perform multiple intermediate operations on it, passing the modified data frame through on each step.\n\npoultry %&gt;%\n  group_by(Product) %&gt;%\n  summarise(mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nVoila!\nLet’s break that down a bit. First, I entered poultry, the name of the tibble I was working with. I then used the pipe, to pass poultry through to the group_by() function, where I created groups based on the column Product. If I run just these first two lines, we can see that poultry looks the same, but R now tells us that there are 5 groups present, based on the variable Product.\n\npoultry %&gt;%\n  group_by(Product)\n\n\n\n  \n\n\n\nFinally, after we group by Product, we can then pass this through to the summarise function using a pipe, where we compute the mean price in the same way as before.\n\npoultry %&gt;%\n  group_by(Product) %&gt;%\n  summarise(mean_price=mean(Price_Dollar,na.rm=T))\n\n\n\n  \n\n\n\nThe pipe is a powerful tool that we will use quite a bit in this class. It does take some getting used to, and I don’t expect it to click right away. With practice, however, you will become proficient at using it."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#conclusion",
    "href": "posts/example-groupby_summarize.html#conclusion",
    "title": "group_by() & summarise()",
    "section": "Conclusion",
    "text": "Conclusion\nNow you’ve learned a little bit about how to summarize data, and in particular how to use group_by() to summarize it based on a grouping variable. We also learned how to create new columns within an existing data frame."
  },
  {
    "objectID": "posts/example-groupby_summarize.html#footnotes",
    "href": "posts/example-groupby_summarize.html#footnotes",
    "title": "group_by() & summarise()",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m not actually sure why you would want to do so. This is just an example of how to use mutate().↩︎\nmutate() can also modify an existing column, but we won’t be using it for that here.↩︎\nAn object is a generic term for any variable in your R environment.↩︎"
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xls ⭐\nFAOstat*.csv or birds.csv ⭐⭐⭐\nhotel_bookings.csv ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a brief, high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set. Describe what you find."
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readxl)\nlibrary(here)\n\n\nhere() starts at /Users/seanconway/Github/DACSS_601_W24\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#challenge-overview",
    "href": "posts/challenge1_solutions.html#challenge-overview",
    "title": "Challenge 1 Solution",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#read-in-the-data",
    "href": "posts/challenge1_solutions.html#read-in-the-data",
    "title": "Challenge 1 Solution",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\nYou should have already downloaded the datasets from Google Classroom and stored them in a common directory on your computer.\nIn this challenge, as in all subsequent challenges, the number of stars corresponds to the difficulty of the dataset. You are only required to do the challenge on one dataset, though you are welcome to do it with multiple datasets.\nIn general, I encourage you to “challenge” yourself by trying to work with a dataset above your experience.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat\\*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation.\n\nRailroad ⭐FAOSTAT / birds⭐⭐Wild Birds ⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad &lt;- here(\"posts\",\"_data\",\"railroad_2012_clean_county.csv\") %&gt;%\n  read_csv()\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the file, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad %&gt;%\n  select(state) %&gt;%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%&gt;%\n  select(state)%&gt;%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estimates of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\nWe’re also lumping in the birds.csv dataset here, because it comes from the same source.\n\nRead the Data\n\n\nCode\nbirds &lt;- here(\"posts\",\"_data\",\"birds.csv\") %&gt;%\n  read_csv()\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens &lt;- here(\"posts\",\"_data\",\"FAOSTAT_egg_chicken.csv\") %&gt;%\n  read_csv()\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncattle &lt;- here(\"posts\",\"_data\",\"FAOSTAT_cattle_dairy.csv\") %&gt;%\n  read_csv()\n\n\nRows: 36449 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncountry &lt;- here(\"posts\",\"_data\",\"FAOSTAT_country_groups.csv\") %&gt;%\n  read_csv()\n\n\nRows: 1943 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Country Group, Country, M49 Code, ISO2 Code, ISO3 Code\ndbl (2): Country Group Code, Country Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nlivestock &lt;- here(\"posts\",\"_data\",\"FAOSTAT_livestock.csv\") %&gt;%\n  read_csv()\n\n\nRows: 82116 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nCode\ncattle\n\n\n\n\n  \n\n\n\nCode\ncountry\n\n\n\n\n  \n\n\n\nCode\nlivestock\n\n\n\n\n  \n\n\n\n\n\nCode\nglimpse(birds)\n\n\nRows: 30,977\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA…\n$ Domain             &lt;chr&gt; \"Live Animals\", \"Live Animals\", \"Live Animals\", \"Li…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5112, 5112, 5112, 5112, 5112, 5112, 5112, 5112, 511…\n$ Element            &lt;chr&gt; \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"…\n$ `Item Code`        &lt;dbl&gt; 1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057, 105…\n$ Item               &lt;chr&gt; \"Chickens\", \"Chickens\", \"Chickens\", \"Chickens\", \"Ch…\n$ `Year Code`        &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Year               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Unit               &lt;chr&gt; \"1000 Head\", \"1000 Head\", \"1000 Head\", \"1000 Head\",…\n$ Value              &lt;dbl&gt; 4700, 4900, 5000, 5300, 5500, 5800, 6600, 6290, 630…\n$ Flag               &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", NA, \"F\", \"F\", \"F…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"FAO estimate\", \"FAO estimate\", \"FA…\n\n\nCode\nglimpse(chickens)\n\n\nRows: 38,170\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL…\n$ Domain             &lt;chr&gt; \"Livestock Primary\", \"Livestock Primary\", \"Livestoc…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5313, 5410, 5510, 5313, 5410, 5510, 5313, 5410, 551…\n$ Element            &lt;chr&gt; \"Laying\", \"Yield\", \"Production\", \"Laying\", \"Yield\",…\n$ `Item Code`        &lt;dbl&gt; 1062, 1062, 1062, 1062, 1062, 1062, 1062, 1062, 106…\n$ Item               &lt;chr&gt; \"Eggs, hen, in shell\", \"Eggs, hen, in shell\", \"Eggs…\n$ `Year Code`        &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Year               &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Unit               &lt;chr&gt; \"1000 Head\", \"100mg/An\", \"tonnes\", \"1000 Head\", \"10…\n$ Value              &lt;dbl&gt; 4000, 25000, 10000, 4400, 25000, 11000, 4600, 25000…\n$ Flag               &lt;chr&gt; \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\"…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"Calculated data\", \"FAO estimate\", …\n\n\nCode\nglimpse(cattle)\n\n\nRows: 36,449\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL\", \"QL…\n$ Domain             &lt;chr&gt; \"Livestock Primary\", \"Livestock Primary\", \"Livestoc…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5318, 5420, 5510, 5318, 5420, 5510, 5318, 5420, 551…\n$ Element            &lt;chr&gt; \"Milk Animals\", \"Yield\", \"Production\", \"Milk Animal…\n$ `Item Code`        &lt;dbl&gt; 882, 882, 882, 882, 882, 882, 882, 882, 882, 882, 8…\n$ Item               &lt;chr&gt; \"Milk, whole fresh cow\", \"Milk, whole fresh cow\", \"…\n$ `Year Code`        &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Year               &lt;dbl&gt; 1961, 1961, 1961, 1962, 1962, 1962, 1963, 1963, 196…\n$ Unit               &lt;chr&gt; \"Head\", \"hg/An\", \"tonnes\", \"Head\", \"hg/An\", \"tonnes…\n$ Value              &lt;dbl&gt; 700000, 5000, 350000, 700000, 5000, 350000, 780000,…\n$ Flag               &lt;chr&gt; \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\", \"Fc\", \"F\", \"F\"…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"Calculated data\", \"FAO estimate\", …\n\n\nCode\nglimpse(country)\n\n\nRows: 1,943\nColumns: 7\n$ `Country Group Code` &lt;dbl&gt; 5100, 5100, 5100, 5100, 5100, 5100, 5100, 5100, 5…\n$ `Country Group`      &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\",…\n$ `Country Code`       &lt;dbl&gt; 4, 7, 53, 20, 233, 29, 35, 32, 37, 39, 24, 45, 46…\n$ Country              &lt;chr&gt; \"Algeria\", \"Angola\", \"Benin\", \"Botswana\", \"Burkin…\n$ `M49 Code`           &lt;chr&gt; \"012\", \"024\", \"204\", \"072\", \"854\", \"108\", \"132\", …\n$ `ISO2 Code`          &lt;chr&gt; \"DZ\", \"AO\", \"BJ\", \"BW\", \"BF\", \"BI\", \"CV\", \"CM\", \"…\n$ `ISO3 Code`          &lt;chr&gt; \"DZA\", \"AGO\", \"BEN\", \"BWA\", \"BFA\", \"BDI\", \"CPV\", …\n\n\nCode\nglimpse(livestock)\n\n\nRows: 82,116\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA…\n$ Domain             &lt;chr&gt; \"Live Animals\", \"Live Animals\", \"Live Animals\", \"Li…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5111, 5111, 5111, 5111, 5111, 5111, 5111, 5111, 511…\n$ Element            &lt;chr&gt; \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"…\n$ `Item Code`        &lt;dbl&gt; 1107, 1107, 1107, 1107, 1107, 1107, 1107, 1107, 110…\n$ Item               &lt;chr&gt; \"Asses\", \"Asses\", \"Asses\", \"Asses\", \"Asses\", \"Asses…\n$ `Year Code`        &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Year               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Unit               &lt;chr&gt; \"Head\", \"Head\", \"Head\", \"Head\", \"Head\", \"Head\", \"He…\n$ Value              &lt;dbl&gt; 1300000, 851850, 1001112, 1150000, 1300000, 1200000…\n$ Flag               &lt;chr&gt; NA, NA, NA, \"F\", NA, NA, NA, NA, NA, NA, NA, NA, \"F…\n$ `Flag Description` &lt;chr&gt; \"Official data\", \"Official data\", \"Official data\", …\n\n\nThere’s clearly a lot going on with these data, but using the glimpse() function, as well as scrolling through the data, allows us to get more of a handle on the datasets. The columns containing Code appear to be redundant with another column, so we can likely ignore them for now.\nFor now, we can focus on birds, cattle, chickens, and livestock.\nIn all 4 of these datasets, the Area column indicates the location of the agricultural product. Element indicates the type of product, Item indicates the animal. Year indicates the year of estimate. Unit indicates the unit of measurement.\nThese data are in a long format - technically I might classify them as “extra long”. Don’t worry if you don’t know what this means, yet - we haven’t gotten there in the course. Each row appears to compromise a case - a single measurement. A single row will contain a measurement of an agricultural product from a single year and in a single country.\nThe country dataset is less interesting. It only contains the codes to match Country and Country groups. We may need to go to the FAOSTAT website to figure this out more. For now, we’ll move on to the next dataset.\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in than an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;%\n  read_excel()\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip argument from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;%\n  read_excel(skip = 1)\nwildbirds \n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds &lt;- here(\"posts\",\"_data\",\"wild_bird_data.xlsx\") %&gt;% \n  read_excel(skip = 2,col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\nThe data are pretty straightforward to interpret.\n\n\nCode\nglimpse(wildbirds)\n\n\nRows: 146\nColumns: 2\n$ weight   &lt;dbl&gt; 5.458872, 7.764568, 8.638587, 10.689735, 7.417226, 9.116935, …\n$ pop_size &lt;dbl&gt; 532194.3951, 3165107.4454, 2592996.8678, 3524193.2266, 389806…\n\n\nEach row is a single case, with measurements of weight and population size for (presumably) a single species of bird.\nWe may need to take a look at the publication if we want to figure out the species’ name. This is above and beyond this challenge, so we will move on.\n## Railroad (xls) ⭐⭐⭐⭐\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nhere(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(skip=3)\n\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nhere(\"posts\",\"_data\",\"StateCounty2012.xls\")  %&gt;%\n  read_excel(skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%&gt;%\n  select(-contains(\"delete\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(skip = 4,col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%&gt;%\n  select(!contains(\"delete\"))%&gt;%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad &lt;- head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nThe range approach\nWe can manually specify the range of cells we want to read in using the range argument. To do so, you’ll need to open the file up in Excel (or a similar program) and figure this out on your own.\n\n\nCode\nrailroad_new &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(range = \"B4:F2990\", col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\")) %&gt;%\n  select(!contains(\"delete\"))%&gt;%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\n\nCode\nrailroad_new\n\n\n\n\n  \n\n\n\nCode\ntail(railroad_new,10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%&gt;%\n  select(State)%&gt;%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%&gt;%\n  select(State)%&gt;%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/example-pivoting.html",
    "href": "posts/example-pivoting.html",
    "title": "pivoting/tidy data",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = T)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/example-pivoting.html#overview",
    "href": "posts/example-pivoting.html#overview",
    "title": "pivoting/tidy data",
    "section": "Overview",
    "text": "Overview\nToday we’re going to talk about tidy data, and how to use pivot_longer() and pivot_wider(). It may not be clear exactly why we’re doing things this way, but it will become explicitly clear when we begin data visualization."
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer() or pivot_wider()."
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organiceggpoultry.xls ⭐⭐⭐\naustralian_marriage*.xls ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.xlsx 🌟🌟🌟🌟🌟\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”."
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current dataset and identify how you want a new one to look.\n\nExample: pivoting a dataset\nLets see if this works with a simple example.\n\n\nCode\ndf&lt;-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Mexico   1980 NAFTA    1240.     982.\n2 USA      1990 NAFTA    1157.    2452.\n3 France   1980 EU        772.     770.\n4 Mexico   1990 NAFTA     880.    1865.\n5 USA      1980 NAFTA     663.     984.\n6 France   1990 EU       1588.    2207.\n\n\nIn this example, we want the names outgoing and ingoing to be in a single column indicating the trade direction, with the values being stored in a column indicating the value of the trade."
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data!\n\nExample\n\n\nCode\ndf1&lt;-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf1\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1 Mexico   1980 NAFTA outgoing              1240.\n 2 Mexico   1980 NAFTA incoming               982.\n 3 USA      1990 NAFTA outgoing              1157.\n 4 USA      1990 NAFTA incoming              2452.\n 5 France   1980 EU    outgoing               772.\n 6 France   1980 EU    incoming               770.\n 7 Mexico   1990 NAFTA outgoing               880.\n 8 Mexico   1990 NAFTA incoming              1865.\n 9 USA      1980 NAFTA outgoing               663.\n10 USA      1980 NAFTA incoming               984.\n11 France   1990 EU    outgoing              1588.\n12 France   1990 EU    incoming              2207.\n\n\nYes, once it is pivoted long, it will be much easier to work with."
  },
  {
    "objectID": "posts/challenge3_instructions.html#your-challenge",
    "href": "posts/challenge3_instructions.html#your-challenge",
    "title": "Challenge 3 Instructions",
    "section": "Your challenge",
    "text": "Your challenge\nIn general, try your best with this challenge. You might have a tough time exactly calculating the desired dimensions of your dataset, and that is okay. However, you should be able to do some pivoting and get your data to a tidy format (or at least close to one)."
  },
  {
    "objectID": "posts/example-pivoting.html#tidy-data",
    "href": "posts/example-pivoting.html#tidy-data",
    "title": "pivoting/tidy data",
    "section": "tidy data",
    "text": "tidy data\nAccording to Wickham & Grolemund, there are three principles for tidy data.\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThis is often easier said than done. However, we’re going to get a little bit of practice tidying data using the tidyr functions pivot_longer() and pivot_wider()."
  },
  {
    "objectID": "posts/example-pivoting.html#read-in-data",
    "href": "posts/example-pivoting.html#read-in-data",
    "title": "pivoting/tidy data",
    "section": "Read in data",
    "text": "Read in data\n\nlivestock &lt;- here(\"posts\",\"_data\",\"FAOSTAT_livestock.csv\") %&gt;%\n  read_csv()\n\nRows: 82116 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/example-pivoting.html#pivoting",
    "href": "posts/example-pivoting.html#pivoting",
    "title": "pivoting/tidy data",
    "section": "Pivoting",
    "text": "Pivoting\nWe’re going to practice with the FAOSTAT_livestock.csv dataset.\n\nlivestock\n\n\n\n  \n\n\n\nFirst, we’re going to remove the Code columns. We can get the same information out of the corresponding “non-code” columns, and this cleans things up for us. I will also remove the Flag Column.\nFinally, if we use the unique() function to identify all the unique values in a column, we see that every value of Unit is “Head”, every value of Domain is “Live Animals”, and every value of Element is “Stocks”. If every value is the same, we can remove these columns.\n\nunique(livestock$Unit)\n\n[1] \"Head\"\n\nunique(livestock$Domain)\n\n[1] \"Live Animals\"\n\nunique(livestock$Element)\n\n[1] \"Stocks\"\n\nlivestock_1 &lt;- livestock %&gt;%\n  select(c(-contains(\"Code\"),\n           -contains(\"Flag\"),\n           -Unit,\n           -Domain,\n           -Element)) # use the select() function to choose the columns I'm keeping, contains() to pick the columns containing the string \"Code\", and the - operator to tell select() that I want all the columns that DO NOT contain \"Code\" in their titles\n# also use - operator to remove Flag & Flag Description\n# also use - operator to remove Head\n# also use - operator to remove Domain\nlivestock_1\n\n\n\n  \n\n\n\nHere, we notice that there are observations scattered across multiple rows. For example, if we sort by Year, we can see that each Item column contains the livestock being measured, and Value contains the amount of livestock. This should probably be spread across columns - i.e,. one column for Camel counts, another for Goat counts, etc.\n\nlivestock_1 %&gt;%\n  arrange(Year)\n\n\n\n  \n\n\n\nThat is, the data are too long. We can fix this by using pivot_wider() to take these observations and spread them across columns.\n\nlivestock_2 &lt;- livestock_1 %&gt;%\n  pivot_wider(names_from = Item,\n              values_from = Value)\nlivestock_2\n\n\n\n  \n\n\n\nAbove, I used pivot_wider() to create columns for each unique type of livestock, where the rows contain the amounts of each livestock in a country during a specific year.\nI can use pivot_longer() to get it back to the previous format. Note that I’m always creating a data frame /tibble with a new name rather than modifying an existing one.\n\nlivestock_3 &lt;- livestock_2 %&gt;%\n  pivot_longer(c(Asses,Camels,Cattle,Goats,Horses,Mules,Sheep,Buffaloes,Pigs), # oh my!\n  names_to = \"Item\", # the column containing these livestock names\n  values_to = \"Value\")# the column containing these livestock counts \nlivestock_3               \n\n\n\n  \n\n\n\nThere we are! However, note that if we compare livestock_3 and livestock_1, the former has a lot more rows than the latter. This is because livestock_1 had both explicit and implicit missing data. That is, some items were marked as NA, and others were not listed. When we pivoted wider, these implicit values became explicit, and this explicitness remained when we pivoted longer again."
  },
  {
    "objectID": "posts/example-pivoting.html#conclusion",
    "href": "posts/example-pivoting.html#conclusion",
    "title": "pivoting/tidy data",
    "section": "Conclusion",
    "text": "Conclusion\nNow you know a little bit about pivoting & tidy data. It takes a lot of practice, and we’ll work on it more this semester."
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.xlsx or organiceggpoultry.xls⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions.xlsx ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge5_instructions.html",
    "href": "posts/challenge5_instructions.html",
    "title": "Challenge 5 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#challenge-overview",
    "href": "posts/challenge5_instructions.html#challenge-overview",
    "title": "Challenge 5 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#read-in-data",
    "href": "posts/challenge5_instructions.html#read-in-data",
    "title": "Challenge 5 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ncereal.csv ⭐\nTotal_cost_for_top_15_pathogens_2018.xlsx ⭐\nAustralian Marriage ⭐⭐\nAB_NYC_2019.csv ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐\nPublic School Characteristics ⭐⭐⭐⭐\nUSA Households ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "title": "Challenge 5 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge5_instructions.html#univariate-visualizations",
    "href": "posts/challenge5_instructions.html#univariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations"
  },
  {
    "objectID": "posts/challenge5_instructions.html#bivariate-visualizations",
    "href": "posts/challenge5_instructions.html#bivariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge6_instructions.html",
    "href": "posts/challenge6_instructions.html",
    "title": "Challenge 6 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#challenge-overview",
    "href": "posts/challenge6_instructions.html#challenge-overview",
    "title": "Challenge 6 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least one graph including time (evolution)\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nCreate at least one graph depicting part-whole or flow relationships\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#read-in-data",
    "href": "posts/challenge6_instructions.html#read-in-data",
    "title": "Challenge 6 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ndebt ⭐\nfed_rate ⭐⭐\nabc_poll ⭐⭐⭐\nusa_hh ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\nAB_NYC ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "title": "Challenge 6 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge6_instructions.html#time-dependent-visualization",
    "href": "posts/challenge6_instructions.html#time-dependent-visualization",
    "title": "Challenge 6 Instructions",
    "section": "Time Dependent Visualization",
    "text": "Time Dependent Visualization"
  },
  {
    "objectID": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "href": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "title": "Challenge 6 Instructions",
    "section": "Visualizing Part-Whole Relationships",
    "text": "Visualizing Part-Whole Relationships"
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html",
    "href": "posts/example-data_analysis_pipeline.html",
    "title": "Data analysis pipeline",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(here)\nlibrary(readxl)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#overview",
    "href": "posts/example-data_analysis_pipeline.html#overview",
    "title": "Data analysis pipeline",
    "section": "Overview",
    "text": "Overview\nSo far, we have discussed aspects of the data pipeline in isolation. Today, we’re going to put all of that together and discuss how to make all of this into a coherent and reproducible process.\nWe’re going to do so using a dataset we’ve worked with before StateCounty2012.xls.\nThis analysis will focus on looking at employee counts & proportions within and across US states.\nNote that this analysis will somewhat overlap with challenge 2, but that is okay. Challenge 2 is more about computing descriptive statistics, while this analysis is more about creating a systematic and coherent data pipeline."
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#data-import",
    "href": "posts/example-data_analysis_pipeline.html#data-import",
    "title": "Data analysis pipeline",
    "section": "Data Import",
    "text": "Data Import\nFirst, we begin by importing the data. We’ve discussed this before (see the Challenge 1 solutions post for more detail), but we’ll need to do some work to get this data imported. Here, I’m using the \"range\" argument of read_excel() to specify the cells I want to read in.\n\nrailroad_initial &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(range=\"B5:F2990\",col_names=c(\"state\",\"delete\",\"county\",\"delete\",\"total_employees\"))\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\nrailroad_initial\n\n\n\n  \n\n\n\nAs we did in Challenge 1, we’ll remove those delete columns and filter out the rows where \"total\" is found.\n\nrailroad &lt;- railroad_initial %&gt;%\n  filter(str_detect(state,\"Total\",negate=T)) %&gt;%\n  select(!contains(\"delete\"))\nrailroad"
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#a-bit-more-data-wrangling",
    "href": "posts/example-data_analysis_pipeline.html#a-bit-more-data-wrangling",
    "title": "Data analysis pipeline",
    "section": "A bit more data wrangling",
    "text": "A bit more data wrangling\nBefore we start doing more analysis, we need to do some more wrangling.\nThe railroad data frame contains employee counts not only from the 50 US states but also from military designations and Canada. For our purposes, we are going to focus on the 50 US states. This means that we need a principled way to filter out these other rows from our dataset.\nLuckily, the datasets package, which comes pre-loaded in R, contains a vector of all state abbreviations, named state.abb.\n\nstate.abb\n\n [1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"FL\" \"GA\" \"HI\" \"ID\" \"IL\" \"IN\" \"IA\"\n[16] \"KS\" \"KY\" \"LA\" \"ME\" \"MD\" \"MA\" \"MI\" \"MN\" \"MS\" \"MO\" \"MT\" \"NE\" \"NV\" \"NH\" \"NJ\"\n[31] \"NM\" \"NY\" \"NC\" \"ND\" \"OH\" \"OK\" \"OR\" \"PA\" \"RI\" \"SC\" \"SD\" \"TN\" \"TX\" \"UT\" \"VT\"\n[46] \"VA\" \"WA\" \"WV\" \"WI\" \"WY\"\n\n\nWe can use the %in% operator, which returns TRUE if a value is found within a vector, to filter out non-states.\n\n\"b\" %in% c(\"a\",\"b\",\"c\")\n\n[1] TRUE\n\n\"b\" %in% c(\"x\",\"y\",\"z\")\n\n[1] FALSE\n\n\n\nrailroad_us &lt;- railroad %&gt;%\n  filter(state %in% state.abb)\nrailroad_us\n\n\n\n  \n\n\n\nLet’s double-check to make sure we have all 50 states.\n\nrailroad_us %&gt;%\n  summarise(n=n_distinct(state))\n\n\n\n  \n\n\n\nYes! It all worked, and we can start analyzing the data."
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#analysis",
    "href": "posts/example-data_analysis_pipeline.html#analysis",
    "title": "Data analysis pipeline",
    "section": "Analysis",
    "text": "Analysis\nFor this analysis, we are going to focus on exploring the data.\nThe first step of this analysis will be to get state totals of employees. Below, we use group_by() and summarise() to add up the employees within a state and put them in a new column named total.\n\nrailroad_state_totals &lt;- railroad_us %&gt;%\n  group_by(state) %&gt;%\n  summarise(total=sum(total_employees)) %&gt;%\n  ungroup() # always use ungroup!!!\nrailroad_state_totals\n\n\n\n  \n\n\n\nNow that we have these totals, we’re going to calculate some measures of central tendency and dispersion. Central tendency refers to the “middle” of the variable (roughly speaking) and is commonly measured using mean or median. Dispersion refers to the extent to which the data differ from the center and is commonly measured using standard deviation and range. We’ll also use min() and max() to find the the minimum and maximum number of employees in a state.\n\nrailroad_state_totals %&gt;%\n  summarise(mean=mean(total),\n            median=median(total),\n            sd=sd(total),\n            range=max(total)-min(total),\n            min=min(total),\n            max=max(total))\n\n\n\n  \n\n\n\nNote that the mean is substantially higher than the median! This indicates that the data are somewhat skewed - essentially that there could be a few states with high employee totals that are driving up the average.\nLet’s take a closer look at the data to see if we can’t find those states with high numbers of employees. We can use arrange() to sort the data based on total, and then slice() to pick out the top 5 rows.\n\nrailroad_state_totals %&gt;%\n  arrange(desc(total)) %&gt;%\n  slice(1:5)\n\n\n\n  \n\n\n\nWe can see that there are actually three states with substantially high employee totals - Texas, Illinois, and New York.\nLet’s take a closer look at those states in railroad_us. It seems highly possible that, within those states, there are some counties that have a much higher proportion of employees. For example, Illinois’s Cook County, which contains the city of Chicago, almost certainly has a high proportion of railroad employees.\nBelow, we use group_by() and mutate() to create a new column that contains the total employees within each state1, called n. This is the denominator that will allow us to compute the proportion of each state’s employees that come from a particular county. Then, we use mutate() to compute these proportions and then arrange the data descending based on proportions to see if there are any high proportion counties.\n\nrailroad_us %&gt;%\n  filter(state %in% c(\"TX\",\"IL\",\"NY\")) %&gt;%\n  group_by(state) %&gt;%\n  mutate(n=sum(total_employees)) %&gt;%\n  ungroup() %&gt;%\n  mutate(prop=total_employees/n) %&gt;%\n  arrange(desc(prop))\n\n\n\n  \n\n\n\nIndeed, Cook County in Illinois (which contains Chicago), Suffolk County in New York (which contains much of Long Island), and Tarrant County in Texas (which contains Fort Worth), all have high proportions of their states’ railroad employees. These are all populous areas that likely have high amounts of commercial and passenger traffic.\nThat concludes our analysis for now. The next step would be to create visualizations, though we haven’t gotten there in the course."
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#conclusion",
    "href": "posts/example-data_analysis_pipeline.html#conclusion",
    "title": "Data analysis pipeline",
    "section": "Conclusion",
    "text": "Conclusion\nEven with a fairly simple dataset like StateCounty2012, there is plenty of analysis to be done! In this example, we read in a problematic data file, filtered out cases that weren’t related to our research question, and identified interesting patterns for our dataset. We did so entirely within a coherent data pipeline."
  },
  {
    "objectID": "posts/example-data_analysis_pipeline.html#footnotes",
    "href": "posts/example-data_analysis_pipeline.html#footnotes",
    "title": "Data analysis pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe also could have used _join() function to combine the railroad_us and railroad_state_totals data frames. We haven’t gotten to joins yet, so I’ll save that for a later date.↩︎"
  },
  {
    "objectID": "posts/challenge2_solutions.html",
    "href": "posts/challenge2_solutions.html",
    "title": "Challenge 2 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(here)"
  },
  {
    "objectID": "posts/challenge2_solutions.html#challenge-overview",
    "href": "posts/challenge2_solutions.html#challenge-overview",
    "title": "Challenge 2 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics\n\n\nRailroad ⭐FAOstat* ⭐⭐⭐Hotel Bookings ⭐⭐⭐⭐\n\n\nThe railroad data contain 2931 county-level aggregated counts of the number of railroad employees in 2012. Counties are embedded within States, and all 50 states plus Canada, overseas addresses in Asia and Europe, and Washington, DC are represented.\n\nRead the data\nHere, we are just reusing the code from Challenge 1. We are using the excel version, to ensure that we get Canada, and are renaming the missing data in county for Canada so that we don’t accidentally filter that observation out.\n\nrailroad &lt;- here(\"posts\",\"_data\",\"StateCounty2012.xls\") %&gt;%\n  read_excel(skip = 4, col_names= c(\"state\", \"delete\",  \"county\",\n                                  \"delete\", \"employees\"))%&gt;%\n  select(!contains(\"delete\"))%&gt;%\n  filter(!str_detect(state, \"Total\"))\n\nNew names:\n• `delete` -&gt; `delete...2`\n• `delete` -&gt; `delete...4`\n\nrailroad&lt;-head(railroad, -2)%&gt;%\n  mutate(county = ifelse(state==\"CANADA\", \"CANADA\", county))\n\nrailroad\n\n\n\n  \n\n\n\n\n\nHow many values does X take on?\nNow, lets practice grouping our data and using other dplyr commands that make data wrangling super easy. First, lets take a closer look at how we counted the number of unique states last week. First, we selected the state column. Then we used the n_distinct command - which replicates the base R commands length(unique(var)).\n\n\n\n\n\n\nacross()\n\n\n\nInstead of counting the number of distinct values one at a time, I am doing an operation on two columns at the same time using across.\n\n\n\nrailroad%&gt;%\n  summarise(across(c(state,county), n_distinct))\n\n\n\n  \n\n\n\nCheck this out - many counties have the same name! There are 2931 state-county cases, but only 1710 distinct county names. This is one reason it is so critical to understand “what is a case” when you are working with your data - otherwise you might accidentally collapse or group information that isn’t intended to be grouped.\n\n\nHow many total X are in group Y?\nSuppose we want to know the total number of railroad employees was in 2012, what is the best way to sum up all of the values in the data? The summarize function is useful for doing calculations across some or all of a data set.\n\nrailroad %&gt;%\n  summarise(total_employees = sum(employees))\n\n\n\n  \n\n\n\nAround a quarter of a million people were employed in the railroad industry in 2012. While this may seem like a lot, it was a significant decrease in employment from a few decades earlier, according to official Bureau of Labor Statistics (BLS) estimates.\nYou may notice that the BLS estimates are significantly lower than the ones we are using, provided by the Railroad Retirement Board. Given that the Railroad Retirement Board has “gold-standard” data on railroad employees, this discrepancy suggests that many people who work in the railroad industry are being classified in a different way by BLS statistics.\n\n\nWhich X have the most Y?\nSuppose we are interested in which county names are duplicated most often, or which states have the most railroad employees. We can use the same basic approach to answer both “Which X have the most Y?” style questions.\n\n\n\n\n\n\ndf-print: paged (YAML)\n\n\n\nWhen you are using df-print: paged in your yaml header, or are using tibbles, there is no need to rely on the head(data) command to limit your results to the top 10 of a list.\n\n\n\nrailroad %&gt;%\n  group_by(state)%&gt;%\n  summarise(total_employees = sum(employees),\n            num_counties = n())%&gt;%\n  arrange(desc(total_employees))\n\n\n\n  \n\n\n\nLooking at the top 10 states in terms of total railroad employment, a few trends emerge. Several of the top 10 states with large numbers of railroad employment are highly populous and geographically large. California, Texas, New York, Pennsylvania, Ohio, Illinois, and Georgia are all amonst the top-10 largest states - so it would make sense if there are more railroad employees in large states.\nBut railroads are spread out along geography, and thus we might also expect square mileage within a state to be related to state railroad employment - not just state population. For example, Texas is around 65% larger (in area) than California, and has around 50% more railroad employees.\nThere appear to be multiple exceptions to both rules, however. If geography plus population were the primary factors explaining railroad employment, then California would be ranked higher than New York and Illinois, and New York would likely rank higher than Illinois. However, Illinois - Chicago in particular - is a hub of railroad activity, and thus Illinois’ higher ranking is likely reflecting hub activity and employment. New York is a hub for the East Coast in particular. While California may have hubs of train activity in Los Angeles or San Francisco, the Northeast has a higher density of train stations and almost certainly generates more passenger and freight miles than the larger and more populous California.\nThis final factor - the existence of heavily used train routes probably explains the high railroad employment in states like Nebraska, Indiana and Missouri - all of which lay along a major railway route between New York and Chicago, and then out to California. Anyway who has played Ticket to Ride probably recognizes many of these routes!\n\n\n\nThe FAOSTAT sheets are excerpts of the FAOSTAT database provided by the Food and Agriculture Association, an agency of the United Nations. We are using the file birds.csv that includes estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) for 248 areas for 58 years between 1961-2018. Estimated stocks are given in 1000 head.\nBecause we know (from challenge 1) that several of those areas include aggregated data (e.g., ) we are going to remove the aggregations, remove the unnecessary variables, and only work with the grouping variables available in the data. In a future challenge, we will join back on more data from the FAO to recreate regional groupings.\n\nbirds &lt;- here(\"posts\",\"_data\",\"birds.csv\") %&gt;%\n  read_csv()%&gt;%\n  select(-c(contains(\"Code\"), Element, Domain, Unit))%&gt;%\n  filter(Flag!=\"A\") %&gt;%\n  select(-c(Flag,`Flag Description`))\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbirds\n\n\n\n  \n\n\n\n\nWhat is the average of Y for X groups?\nLets suppose we are starting off and know nothing about poultry stocks around the world, where could we start? Perhaps we could try to get a sense of the relative sizes of stocks of each of the five types of poultry, identified in the variable Item. Additionally, because some of the values may be missing, lets find out how many of the estimates are missing.\nWe will also compute the standard deviation for each item, to get a sense of the variability in the data.\n\nbirds %&gt;%\n  group_by(Item)%&gt;%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE),\n            sd_stock = sd(Value,na.rm=TRUE),\n            n_missing = sum(is.na(Value)))\n\n\n\n  \n\n\n\nOn average, we can see that countries have far more chickens as livestock (\\(\\bar{x}\\)=58.4million head) than other livestock birds (average stocks range between 2 and 10 million head). However, the information from the median stock counts suggest that there is significant variation across countries along with a strong right hand skew with regards to chicken stocks. The median number of chickens in a country is 3.8 million head - significantly less than the mean of almost 60 million. Overall, missing data doesn’t seem to be a huge issue, so we will just use na.rm=TRUE and not worry too much about the missingness for now.\nAlso, Chicken stock appears to be substantially more variable than that of other items. Though it could be because there is simply more data on Chicken stocks.\nIt could be that stock head counts have changed over time, so lets try selecting two points in time and seeing whether or not average livestock counts are changing.\n\n\n\n\n\n\npivot-wider\n\n\n\nIt can be difficult to visually report data in tidy format. For example, it is tough to compare two values when they are on different rows. In this example, I use pivot-wider to swap a tidy grouping variable into multiple columns to be more “table-like.” I then do some manual formatting to make it easy to compare the grouped estimates.\n\n\n\nt1&lt;-birds%&gt;%\n  filter(Year %in% c(1966, 2016))%&gt;% # less efficiently: filter(Year==1966 | Year == 2016)\n  group_by(Year, Item)%&gt;%\n  summarise(avg_stocks = mean(Value, na.rm=TRUE),\n            med_stocks = median(Value, na.rm=TRUE))%&gt;%\n  pivot_wider(names_from = Year, values_from = c(avg_stocks, med_stocks))\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\nt1\n\n\n\n  \n\n\n\nSure enough, it does look like stocks have changed significantly over time. The expansion of country-level chicken stocks over five decades between 1966 and 2016 are most noteworthy, with both average and median stock count going up by a factor of 4. Pigeons have never been very popular, and average stocks have actually decreased over the same time period while the other less popular bird - turkeys - saw significant increases in stock count. Some countries increased specialization in goose and/or guinea fowl production, as the average stock count went up but the median went down over the same period.\n\n\n\nThis data set contains 119,390 hotel bookings from two hotels (“City Hotel” and “Resort Hotel”) with an arrival date between July 2015 and August 2017 (more detail needed), including bookings that were later cancelled. Each row contains extensive information about a single booking:\n\nthe booking process (e.g., lead time, booking agent, deposit, changes made)\nbooking details (e.g., scheduled arrival date, length of stay)\nguest requests (e.g., type of room, meal(s) included, car parking)\nbooking channel (e.g., distribution, market segment, corporate affiliation for )\nguest information (e.g., child/adult, passport country)\nguest prior bookings (e.g., repeat customer, prior cancellations)\n\nThe data are a de-identified extract of real hotel demand data, made available by the authors.\n\nRead and make sense of the data\nThe hotel bookings data set is new to challenge 2, so we need to go through the same process we did during challenge 1 to find out more about the data. Lets read in the data and use the summmaryTools package to get an overview of the data set.\n\nbookings &lt;- here(\"posts\",\"_data\",\"hotel_bookings.csv\") %&gt;% read_csv()\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbookings\n\n\n\n  \n\n\n\nWow - there is a lot of information available here. Lets scan it and see what jumps out. First we can see that the summary function claims that there are almost 32,000 duplicates in the data. However, this is likely an artifact of the way that the bookings have been de-identified, and may reflect bookings with identical details but different individuals who made the bookings.\n\nMissing data\nFirst, let’s examine missing data:\n\nbookings %&gt;%\n  summarise(across(everything(),~sum(is.na(.x)))) %&gt;%\n  pivot_longer(everything()) %&gt;%\n  arrange(desc(value))\n\n\n\n  \n\n\n\nThis method also works (though not as tidy):\n\nsapply(bookings, function(x) sum(is.na(x)))\n\n                         hotel                    is_canceled \n                             0                              0 \n                     lead_time              arrival_date_year \n                             0                              0 \n            arrival_date_month       arrival_date_week_number \n                             0                              0 \n     arrival_date_day_of_month        stays_in_weekend_nights \n                             0                              0 \n          stays_in_week_nights                         adults \n                             0                              0 \n                      children                         babies \n                             4                              0 \n                          meal                        country \n                             0                              0 \n                market_segment           distribution_channel \n                             0                              0 \n             is_repeated_guest         previous_cancellations \n                             0                              0 \nprevious_bookings_not_canceled             reserved_room_type \n                             0                              0 \n            assigned_room_type                booking_changes \n                             0                              0 \n                  deposit_type                          agent \n                             0                              0 \n                       company           days_in_waiting_list \n                             0                              0 \n                 customer_type                            adr \n                             0                              0 \n   required_car_parking_spaces      total_of_special_requests \n                             0                              0 \n            reservation_status        reservation_status_date \n                             0                              0 \n\n\n\n\n\nBack to the data\nWe are provided with limited information about the hotel. Hotels are identified only as “City” Hotel” or a “Resort Hotel”. Maybe we have bookings from only two hotels? Lets tentatively add that to our data description.\nThere is a flag for whether a booking is cancelled. This means that our universe of cases includes bookings where the guests showed up, as well as bookings that were later cancelled - we can add that to our data description.\nThere are multiple fields with the arrival date - year, month, etc. For now, we can tell that the arrival date of the bookings ranges between 2015 and 2017. More precise identification of the date range could be more easily done next challenge when we can recode the arrival date information using lubridate1.But maybe it is possible to find out which values of month co-occur with specific years?\n\n\nWhich values of Y are nested within X?\nTo approach this question, we can narrow the dataset down to just the two variables of interest, and then use the distinct command.\n\nbookings%&gt;%\n  select(arrival_date_year, arrival_date_month)%&gt;%\n  distinct()\n\n\n\n  \n\n\n\nGreat - now we now that all bookings have arrival dates between June 2015 and August 2017, and can add that to the data description. Just for fun, lets see if we can confirm that the dates are the same for both hotels.\n\n\n\n\n\n\nslice()\n\n\n\nThis would be easier to investigate with proper date variables, but I am using slice to find the first and last row for each hotel, by position. This avoids printing out a long data list we have to scroll through, but would fail if the hotels had different sets of arrival month-year pairs.\n\n\n\nd&lt;-bookings%&gt;%\n  select(arrival_date_year, arrival_date_month)%&gt;%\n  n_distinct\n\nbookings%&gt;%\n  select(hotel, arrival_date_year, arrival_date_month)%&gt;%\n  distinct()%&gt;%\n  slice(c(1, d, d+1, d*2))\n\n\n\n  \n\n\n\nLets suppose we want to know whether or not the two hotels offer the same types of rooms? This is another query of the sort Which values of X are nested in y?\n\nbookings%&gt;%\n  group_by(hotel)%&gt;%\n  count(reserved_room_type)\n\n\n\n  \n\n\n\nIn this case, however, it is tough to directly compare - it appears that the hotel-roomtype pairs are not as consistent as the year-month pairs for the same hotels. A quick pivot-wider makes this comparison a little easier to visualize. Here we can see that the Resort Hotel has two additional room types: “H” and “L”.\n\nbookings%&gt;%\n  group_by(hotel)%&gt;%\n  count(reserved_room_type)%&gt;%\n  pivot_wider(names_from= hotel, values_from = n)\n\n\n\n  \n\n\n\n\n\nWhat is the average of Y for group X?\nThe breakdown of rooms by hotel doesn’t shed much light on the room codes and what they might mean. Lets see if we can find average number of occupants and average price for each room type, and see if we can learn more about our data.\n\n\n\n\n\n\nmean(., na.rm=TRUE)\n\n\n\nI am using the mean function with the option na.rm=TRUE to deal with the four NA values in the children field, identified in the summary table above.\n\n\n\nt1&lt;-bookings%&gt;%\n  group_by(hotel, reserved_room_type)%&gt;%\n  summarise(price = mean(adr),\n            adults = mean(adults),\n            children = mean(children+babies, na.rm=TRUE)\n            )%&gt;%\n  pivot_wider(names_from= hotel, \n              values_from = c(price, adults, children))\n\n`summarise()` has grouped output by 'hotel'. You can override using the\n`.groups` argument.\n\nt1\n\n\n Average Price and Occupancy, by hotel and room type\n  \n\n\n\nBased on these descriptives broken down by hotel and room type, we can speculate that the “H” and “L” room types at the resort are likely some sort of multi-bedroom suite (because the average number of adults is over 2.) Similarly, we can speculate that the difference between ABC and DEF may be something related to room size or quality (e.g., number and size of beds) and/or related to meals included with the rooms - but this would require further investigation to pin down!\n\n\n\n\n\n\nGo further\n\n\n\nThere is lots more to explore in the hotel bookings dataset, but it will be a lot easier once we recode the date fields using lubridate."
  },
  {
    "objectID": "posts/challenge2_solutions.html#footnotes",
    "href": "posts/challenge2_solutions.html#footnotes",
    "title": "Challenge 2 Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that, as a class, we haven’t yet gotten to the lubridate package. So don’t worry too much about this, yet.↩︎"
  },
  {
    "objectID": "posts/challenge3_solutions.html",
    "href": "posts/challenge3_solutions.html",
    "title": "Challenge 3 Solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(here)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE,\n                      message=FALSE, cache=TRUE)"
  },
  {
    "objectID": "posts/challenge3_solutions.html#challenge-overview",
    "href": "posts/challenge3_solutions.html#challenge-overview",
    "title": "Challenge 3 Solutions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nanticipate the shape of pivoted data, and\npivot the data into tidy format using pivot_longer\n\n::: panel-tabset ## Example\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!1\n\nFind current and future data dimensions\nLets see if this works with a simple example.\n\ndf&lt;-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n Example\n  \n\n\n#existing rows/cases\nnrow(df)\n\n[1] 6\n\n#existing columns/cases\nncol(df)\n\n[1] 5\n\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n[1] 12\n\n# expected columns \n3 + 2\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nPivot the data\n\ndf_pivoted&lt;-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf_pivoted\n\n\n Pivoted Example\n  \n\n\n\nYes, once it is pivoted long, our resulting data are \\(12 * 5\\) - exactly what we expected!"
  },
  {
    "objectID": "posts/challenge3_solutions.html#animal-weights",
    "href": "posts/challenge3_solutions.html#animal-weights",
    "title": "Challenge 3 Solutions",
    "section": "Animal Weights ⭐",
    "text": "Animal Weights ⭐\nThe animal weights dataset contains tabular-style data, with cells representing the average live animal weight (in kg) of 16 types of livestock for each of 9 geographic areas as defined by the Intergovernmental Panel on Climate Change (IPCC. Livestock weights are a critical part of the Global Livestock Envrionmental Assessment Model used by the FAO.\n\nanimal_weight&lt;-here(\"posts\",\"_data\",\"animal_weight.csv\") %&gt;%\n  read_csv()\nanimal_weight\n\n\n\n  \n\n\n\nBecause the animal weights data is in tabular format, it is easy to see that \\(n=9\\) regions (categories or cases) in the original data, and that there are \\(k=16\\) types of livestock (categories or columns). Therefore, we expect the pivoted dataset to have \\(9 * 16\\) = 144 rows and 3 columns (region, animal type, and animal weight.)\n\n\n\n\n\n\ninline R code\n\n\n\nIf you check out the code above, you will see that I didn’t use a calculator to figure out \\(9*16=144\\), but used inline r code like this: 144.\n\n\n\nPivot the data\n\nanimal_weight_longer&lt;-pivot_longer(animal_weight, \n                                    col = -`IPCC Area`,\n                                    names_to = \"Livestock\",\n                                    values_to = \"Weight\")\nanimal_weight_longer\n\n\n\n  \n\n\n\nYes, it looks like we ended up with 144 rows and 3 columns, exactly as expected!\n#Go further\nstringr functions, and separate from tidyr, would be useful in helping split out additional information from the Livestock column."
  },
  {
    "objectID": "posts/challenge3_solutions.html#eggs",
    "href": "posts/challenge3_solutions.html#eggs",
    "title": "Challenge 3 Solutions",
    "section": "Eggs ⭐⭐/⭐⭐⭐",
    "text": "Eggs ⭐⭐/⭐⭐⭐\nThis section covers pivoting for the organic eggs data, available in both excel and (partially cleaned) .csv format. The data reports the average price per carton paid to the farmer or producer for organic eggs (and organic chicken), reported monthly from 2004 to 2013. Average price is reported by carton type, which can vary in both size (x-large or large) and quantity (half-dozen or dozen.)\nIf you are using the eggs_tidy.csv, you can skip the first section as your data is in .csv format and already partially cleaned. The first section reviews data read-in and cleaning for the organicpoultry.xls file.\n\nRead and Clean the dataPivot Type OnlyPivot Size and Quantity\n\n\nThere are three sheets in the organicpoultry.xls workbook: one titled Data, one titled “Organic egg prices, 2004-13” and one with a similar title for chicken prices. While I can tell all of this from inspection, I can also use a ask R to return the sheet names for me.\n\n\n\n\n\n\nGet sheet names with excel_sheets()\n\n\n\nBoth readxl and googlesheets4 have a function that can return sheet names as a vector. This is really useful if you need to parse and read multiple sheets in the same workbook.\n\n\n\nhere(\"posts\",\"_data\",\"organiceggpoultry.xls\") %&gt;%\n  excel_sheets()\n\n[1] \"Data\"                            \"Organic egg prices, 2004-13\"    \n[3] \"Organic poultry prices, 2004-13\"\n\n\nWhile it may seem like it would be easier to read in the individual egg prices and chicken prices, the amount of formatting introduced into the second and third sheets is pretty intimidating (see the screenshot below.) There are repeated headers to remove, a year column to shift, and other formatting issues. Ironically, it may be easier to read in the egg data from the Data sheet, with a skip of 5 (to skip the table title, etc), custom column names designed for pivoting to two categories (final section) and only reading in columns B to F.\n\n\n\nOrganic Poultry Data\n\n\n\n\n\nOrganic Poultry Egg Prices\n\n\n\n\n\n\n\n\nHard-coding Table Formats\n\n\n\nFormatted excel tables are a tricky data source, but may be the only way to get some data. If table formatting is consistent from year to year, hard-coding can be an acceptable approach. If table format is inconsistent, then more powerful tools are needed.\n\n\n\neggs_orig&lt;- here(\"posts\",\"_data\",\"organiceggpoultry.xls\") %&gt;%\n  read_excel(sheet=\"Data\",\n             range = \"B6:F125\",\n             col_names = c(\"date\", \n                           \"xlarge_dozen\",\n                           \"xlarge_halfdozen\",\n                           \"large_dozen\",\n                           \"large_halfdozen\")\n  )\neggs_orig\n\n\n\n  \n\n\n\nSometimes there are notes in the first column of tables, so lets make sure that isn’t an issue.\n\neggs_orig%&gt;%\n  count(date)\n\n\n\n  \n\n\n\nWe need to remove the “note” indicators in two of the rows. Some characters require an escape to be included in regular expressions, but this time it is straightforward to find ” /1”.\n\neggs&lt;-eggs_orig%&gt;%\n  mutate(date = str_remove(date, \" /1\"))\n\nOne final step is needed to split the year variable away from the month. You will often need to separate out two variables from a single column when working with published tables, and also need to use the equivalent of dragging to fill in a normal spreadsheet. Lets look at the easiest way to fix both of these issues.\n\n\n\n\n\n\ntidyr::separate()\n\n\n\nSeparate is a fantastic function for working with strings. It will break a string column into multiple new (named) columns, at the indicated separator character (e.g., “,” or ” “). The old variable is automatically removed, but can be left.\n\n\n\n\n\n\n\n\ntidyr::fill()\n\n\n\nFill works like dragging to fill functionality in a spreadsheet. You can choose the direction to fill.\n\n\n\neggs_filled &lt;- eggs%&gt;%\n  separate(date, into=c(\"month\", \"year\"), sep=\" \")%&gt;%\n  fill(year)\neggs_filled\n\n\n\n  \n\n\n\n\n\nLooking at the data, we can see that each of the original 120 cases consist of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) So to tidy our data, we should create a matrix with a year-month-eggType combination, with a single price value for each case.\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\neggs_long&lt;-eggs_filled%&gt;%\n  pivot_longer(cols=contains(\"large\"), \n               names_to = \"egg_type\",\n               values_to = \"avg_price\"\n  )\neggs_long\n\n\n\n  \n\n\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs.\n\n\nWouldn’t it be nice if we had two new columns - size and quantity - in place of the existing eggType categorical variable? In other words, to have fully tidy data, we would need 4 grouping variables (year, month, size, and quantity) and the same value (price). So, we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240).\nHow can we let R know what we want it to do?? Thankfully, we created pretty systematic column names for egg types in our original data, following the general pattern: size-quantity. Maybe we can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it easier than you would think to pivot four existing columns into two new columns.\n\neggs_long&lt;- eggs_filled%&gt;%\n  pivot_longer(cols=contains(\"large\"),\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"_\",\n               values_to = \"price\"\n  )\neggs_long"
  },
  {
    "objectID": "posts/challenge3_solutions.html#australian-marriage-ballot",
    "href": "posts/challenge3_solutions.html#australian-marriage-ballot",
    "title": "Challenge 3 Solutions",
    "section": "Australian Marriage Ballot ⭐⭐⭐",
    "text": "Australian Marriage Ballot ⭐⭐⭐\nThis is another tabular data source published by the Australian Bureau of Statistics that requires a decent amount of cleaning. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage: “Should the law be changed to allow same-sex couples to marry?” All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way, or fail to vote. (See the “Explanatory Notes” sheet for more details.)\nThe provided table includes estimates of the proportion of citizens choosing each of the four options, aggregated by Federal Electoral District, which are nested within one of 8 overarching Electoral Divisions. Here is a quick image showing the original table format.\n\n\n\nAustralian Marriage Data\n\n\n\nIdentify desired data structure\nInspection reveals several critical issues to address: - Typical long header (skip = 7) - No single row with variable names - Two redundant values (count and percentage - percentage is easy to recover from complete count data) - Total columns that are redundant (remove) - The sum of “Yes” and “No” votes appears to be redundant with Response Clear in columns I and J - District and Division are in the same column\nIn this example, we are going to identify the desired structure early in the process, because clever naming of variables makes it much easier to use pivot functions. We will skip reading in redundant data (proportions and “totals” columns), and then can identify four potentially distinct pieces of information. Three grouping variables: Division (in column 1), District (also in column 1), and citizen Response (yes, no, unclear, and non-response), plus one value: aggregated response Count.\nOur basic data reading and cleaning process should therefore follow these steps:\n\nRead in data, skipping unneeded columns and renaming variables\nCreate Division and District variables using separate() and fill()\npivot_longer() four response variables into 2 new Response and Count variables (double the number of rows)\n\n\nRead DataSeparate District and DivisionPivot_longer\n\n\nIt is best to confine serious hard-coding to the initial data read in step, to make it easy to locate and make changes or replicate in the future. So, we will use a combination of tools introduced earlier to read and reformat the data: skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\nvote_orig &lt;- here(\"posts\",\"_data\",\"australian_marriage_law_postal_survey_2017_-_response_final.xls\") %&gt;%\n  read_excel(sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"District\", \"Yes\", \"del\", \"No\", rep(\"del\", 6), \"Illegible\", \"del\", \"No Response\", rep(\"del\", 3)))%&gt;%\n  #select(!starts_with(\"del\"))%&gt;%\n  select(-matches(\"del\")) %&gt;%\n  drop_na(District)%&gt;%\n  filter(!str_detect(District, \"(Total)\"))%&gt;%\n  filter(!str_starts(District, \"\\\\(\"))\nvote_orig\n\n\n\n  \n\n\n\n\n\nThe most glaring remaining issue is that the administrative Division is not in its own column, but is in its own row within the District column. The following code uses case_when to make a new Division variable with an entry (e.g., New South Wales Division) where there is a Division name in the District column, and otherwise it create just an empty space. After that, fill can be used to fill in empty spaces with the most recent Division name. We then filter out rows with only the title information.\n\nvote&lt;- vote_orig%&gt;%\n  mutate(Division = case_when(\n    str_ends(District, \"Divisions\") ~ District,\n    TRUE ~ NA_character_ )) %&gt;%\n  fill(Division, .direction = \"down\") %&gt;%\n  filter(!str_detect(District, \"Division|Australia\"))\nvote\n\n\n\n  \n\n\n\n\n\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn’t vote. Or if we wanted to use division level characteristics to predict the proportion of people voting in a specific way? In both cases, we would need tidy data, which requires us to pivot longer into the original (aggregated) data format: Division, District, Response, Count. We should end up with 600 rows and 4 columns.\n\nvote_long&lt;- vote%&gt;%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  ) %&gt;%\n  mutate(Division=str_remove(Division, \" Divisions\"))\nvote_long"
  },
  {
    "objectID": "posts/challenge3_solutions.html#usa-households",
    "href": "posts/challenge3_solutions.html#usa-households",
    "title": "Challenge 3 Solutions",
    "section": "USA Households ⭐⭐⭐⭐",
    "text": "USA Households ⭐⭐⭐⭐\nThe excel workbook “USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019” is clearly a table of census-type household data (e.g., Current Population Study or CPS, American Community Study or ACS, etc.) Row 3 of the workbook provides a link to more details about the origin of the data used to produce the table.\nThe cases in this example are essentially year-identity groups, where I use the term identity to refer to the wide range of ways that the census can cluster racial and identity. While there are 12 categories in the data, many of these overlap and/or are not available in specific years. For example, one category is “All Races”, and it overlaps with all other categories but cannot be easily eliminated.\n\n\n\nExcel Workbook Screenshot\n\n\n\nIdentify desired data structure\nInspection of the excel workbook reveals several critical features of the data.\n\ncolumn names (of a sort) are in rows 4 and 5 (skip=5 and rename)\nfirst column includes year and race/hispanic origin households\nfirst column appears to have notes of some sort (remove notes)\nthere are end notes starting in row 358 (n_max = 352)\n“Total” column appears to be redundant proportion info\n\nThe data appears to have two grouping variables (year and identity), plus several values:\n\na count of number of households\nmedian and mean income (and associated margin of error)\nproportion of households with hhold income in one of 9 designated ranges or brackets\n\nThe final data should potentially be split into two data tables - one with averages for a group of households, the other with proportions information based on income brackets. There is less clarity about how to handle the potential overlap between race and hispanic group identity information.\n\nRead dataClean and separate yearSanity check identitySplit the data\n\n\nTo make it easy to pivot, we will read in income brackets cleanly, along with clear variable names for the estimated average (and standard error) of hhold income for each designated identity group.\n\nincome_brackets &lt;- c(i1 = \"Under $15,000\",\n                     i2 = \"$15,000 to $24,999\",\n                     i3 = \"$25,000 to $34,999\",\n                     i4= \"$35,000 to $49,999\",\n                     i5 = \"$50,000 to $74,999\",\n                     i6 = \"$75,000 to $99,999\",\n                     i7 = \"$100,000 to $149,999\",\n                     i8 = \"$150,000 to $199,999\",\n                     i9 = \"$200,000 and over\")\n\nushh_orig &lt;- here(\"posts\",\"_data\",\"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\") %&gt;%\n  read_excel(skip=5,\n         n_max = 352,\n         col_names = c(\"year\", \"hholds\", \"del\",\n                       str_c(\"income\",1:9,sep=\"_i\"),\n                       \"median_inc\", \"median_se\", \n                       \"mean_inc\",\"mean_se\"))%&gt;%\n  select(-del)\nushh_orig \n\n\n\n  \n\n\n\n\n\nThe current year column has both year and identity information on the hholds, as well as notes that need to be removed. Because identity labels have spaces, we will need to remove those first before our typical approach to removing notes using separate is going to work.\n\n\n\n\n\n\nRegex and Regexr\n\n\n\nRegular expressions are a critical tool for messy, real world data where you will need to search, replace, and extract information from string variables. Learning regex is tough, but Regexer makes it much easier!\n\n\n\nushh_orig%&gt;%\n  filter(str_detect(year, \"[[:alpha:]]\"))\n\n\n\n  \n\n\n\nNow that we know how to use regular expressions to find the household identity information, we can quickly separate out the identity information from the years, then do the standard fill prior to removing the unneeded category rows.\nOnce that is done, we can use separate to remove the notes from the year column. Removing notes from the identity column is a bit trickier, and requires regex to find cases where there is a space then at least one numeric digit. The notes are important for helping orient yourself to the data, and if I were taking my time, I would document the relevant notes in the text at this point, to remind myself if I need to remember details about how the categories were created and/or why they vary over time.\nNote that several variables are read in as non-numeric, so I’m fixing them in a single statement!\n\nushh_id_tmp&lt;-ushh_orig%&gt;%\n  mutate(identity = case_when(\n    str_detect(year, \"[[:alpha:]]\") ~ year,\n    TRUE ~ NA_character_\n  ))%&gt;%\n  fill(identity)%&gt;%\n  filter(!str_detect(year, \"[[:alpha:]]\"))\n\nushh_id&lt;-ushh_id_tmp%&gt;%\n  separate(year, into=c(\"year\", \"delete\"), sep=\" \")%&gt;%\n  mutate(identity = str_remove(identity, \" [0-9]+\"),\n         across(any_of(c(\"hholds\", \"mean_inc\", \"mean_se\", \"year\")), \n                as.numeric))%&gt;%\n  select(-delete)\n\nushh_id\n\n\n\n  \n\n\n\n\n\nEven from the detailed notes, it is difficult to fully understand what is going on with the identity variable, and whether all of the values are available in every year. A simple sanity check is to pick out several years mentioned in the notes and see if the number of households are available for all categories, and also check to see if there are specific categories that add up to the “all races” category.\n\nushh_id%&gt;%\n  filter(year%in%c(1970, 1972, 1980, 2001, 2002))%&gt;%\n  select(identity, hholds, year)%&gt;%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\nBased on these examples, we can now confirm that the survey did not include a question about Hispanic background prior to 197228, that only “White” and “Black” (and not “Asian”) were systematically recorded prior to 2002, and that other mentioned dates of changes are not relevant to the categories represented in the data. Additionally, we can see from the example years that it would be reasonable to create a consistent time series that collapses the “White” and “White Alone” and “Black” and “Black Alone labels.\nWhile it might appear plausible to create distinct variables for race and hispanic, there is an instructive note in the worksheet that suggests this is probably not possible:\n\nBecause Hispanics may be any race, data in this report for Hispanics overlap with data for racial groups. Hispanic origin was reported by 15.6 percent of White householders who reported only one race, 5.0 percent of Black householders who reported only one race, and 2.5 percent of Asian householders who reported only one race. Data users should exercise caution when interpreting aggregate results for the Hispanic population and for race groups because these populations consist of many distinct groups that differ in socioeconomic characteristics, culture, and recency of immigration. Data were first collected for Hispanics in 1972.\n\nIn fact, when we try to get the various hhouse number estimates to add up to the TOTAL households, it is nearly impossible to do so. For now, the most expeditious and likely acceptable approach may be to simply keep a reasonable set of categories that minimizes category overlaps.\n\n\n\n\n\n\nKeep your original data\n\n\n\nOriginal data that has been carefully documented can be overly detailed and broken into categories that make systematic analysis difficult. When you simplify data categories for exploratory work, keep the original data so that you can reintroduce it at the appropriate point.\n\n\n\nushh &lt;-ushh_id%&gt;%\n  mutate(gp_identity = case_when(\n    identity %in% c(\"BLACK\", \"BLACK ALONE\") ~ \"gp_black\",\n    identity %in% c(\"ASIAN ALONE OR IN COMBINATION\",\n                  \"ASIAN AND PACIFIC ISLANDER\") ~ \"gp_asian\",\n    identity %in% c(\"WHITE, NOT HISPANIC\", \n                    \"WHITE ALONE, NOT HISPANIC\") ~ \"gp_white\",\n    identity %in% c(\"HISPANIC (ANY RACE)\") ~ \"gp_hisp\",\n    identity %in% c(\"ALL RACES\") ~ \"gp_all\"\n  ))%&gt;%\n  filter(!is.na(gp_identity))%&gt;%\n  group_by(year, gp_identity)%&gt;%\n  summarise(across(c(starts_with(\"inc\"),starts_with(\"me\"),\n                     \"hholds\"), \n                   ~sum(.x, na.rm=TRUE)))%&gt;%\n  ungroup()\n\nushh\n\n\n\n  \n\n\n\n\nushh%&gt;%\n  filter(year%in%c(1972, 2001, 2002))%&gt;%\n  select(gp_identity, hholds, year)%&gt;%\n  pivot_wider(values_from=hholds, names_from=year)\n\n\n\n  \n\n\n\n\n\nWe have identified reasonable groups where the parts approximately add up to the whole. They are not perfect, but not horrible given the incomplete and overlapping categories. To get much better, we would need to work with the original ACS or IPUMS microdata.\nNow, there may be times we need to split the data - for example, if we want to graph the income brackets as part of the total households. Lets see if we can split and pivot the data.\n\nushh_brackets &lt;-ushh%&gt;%\n  ungroup()%&gt;%\n  select(year, gp_identity, hholds, starts_with(\"income\"))%&gt;%\n  pivot_longer(cols=starts_with(\"income\"),\n               names_prefix= \"income_\",\n               names_to = \"income_bracket\",\n               values_to = \"percent\")%&gt;%\n  mutate(hholds_bracket = round(hholds*(percent/100)),\n         income_bracket = recode(income_bracket,!!!income_brackets))\n\nushh_brackets"
  },
  {
    "objectID": "posts/challenge3_solutions.html#footnotes",
    "href": "posts/challenge3_solutions.html#footnotes",
    "title": "Challenge 3 Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t expect you to do these calculations on your challenges/homeworks. You can just anticipate the general shape of the data (e.g., the pivoted data will have fewer rows than it does now, and the columns will be…).↩︎"
  }
]